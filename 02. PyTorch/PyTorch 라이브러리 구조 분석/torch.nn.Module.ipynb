{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "22c8fe9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch import nn, Tensor\n",
    "from typing import Any, Callable, Optional, TypeVar, Dict, Union, Tuple\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.hooks import RemovableHandle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac1d65",
   "metadata": {},
   "source": [
    "### nn.Module의 Method\n",
    "- \\_\\_init\\_\\_\n",
    "- \\_forward_unimplemented\n",
    "- register_buffer\n",
    "- register_parameter\n",
    "- add_module\n",
    "- get_submodule\n",
    "- get_parameter\n",
    "- get_buffer\n",
    "- apply\n",
    "- register_forward_pre_hook\n",
    "- register_forward_hook\n",
    "- register_full_backward_hook\n",
    "\n",
    "- \\_\\_call\\_\\_, _call_impl\n",
    "\n",
    "- zero_grad\n",
    "- extra_repr\n",
    "- \\_\\_repr\\_\\_\n",
    "\n",
    "- parameters\n",
    "- named_parameters\n",
    "- buffers\n",
    "- named_buffers\n",
    "- children\n",
    "- named_children\n",
    "- modules\n",
    "- named_modules\n",
    "\n",
    "이 외에도 많은 Method들이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a6c759",
   "metadata": {},
   "source": [
    "### nn.Module 공식 문서의 설명\n",
    "- Base class for all neural network modules.   \n",
    "    \\> 모든 뉴럴네트워크의 모듈들은 nn.Module 을 base class 로 가진다.\n",
    "      \n",
    "      \n",
    "- Your models should also subclass this class.   \n",
    "    \\> 사용자가 만드는 모델들 또한 이 class를 subclass로 가진다\n",
    "\n",
    "\n",
    "- Modules can also contain other Modules, allowing to nest them in a tree structure  \n",
    "    \\> Modules는 다른 Modules를 포함할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f685b0a",
   "metadata": {},
   "source": [
    "### 1. \\_\\_init\\_\\_( )\n",
    "- torch.\\_C.\\_log\\_api\\_usage\\_once : 이 부분은 log를 남기는 것으로 Facebook으로 가는 것으로 [추정된다.](https://discuss.pytorch.org/t/what-does-torch-c-log-api-usage-once-do/137732)\n",
    "\n",
    "\n",
    "- 다른 class의 \\_\\_init\\_\\_ 의 역할 처럼 생성자의 역할을 하여준다.\n",
    "\n",
    "\n",
    "- training의 default 값은 True이다.\n",
    "\n",
    "\n",
    "- parameters, buffers, backward_hooks, forward_hooks, forward_pre_hooks등은 모두 비어있는 OrderDict()를 할당해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "450172d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self) -> None:\n",
    "    \"\"\"\n",
    "    Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
    "    \"\"\"\n",
    "    torch._C._log_api_usage_once(\"python.nn_module\")\n",
    "\n",
    "    self.training = True\n",
    "    self._parameters: Dict[str, Optional[Parameter]] = OrderedDict()\n",
    "    self._buffers: Dict[str, Optional[Tensor]] = OrderedDict()\n",
    "    self._non_persistent_buffers_set: Set[str] = set()\n",
    "    self._backward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    self._is_full_backward_hook = None\n",
    "    self._forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    self._forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    self._state_dict_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    self._load_state_dict_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    self._modules: Dict[str, Optional['Module']] = OrderedDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7366380c",
   "metadata": {},
   "source": [
    "> <code>self._forward_hooks: Dict[int, Callable] = OrderedDict()</code> 같은 구조는 typing 모듈로 타입 표시하는 방법 중 하나이다. [참고](https://www.daleseo.com/python-typing/)  \n",
    "즉 forward_hooks는 int를 key로 Callable한 value를 가져야만한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3a8b90",
   "metadata": {},
   "source": [
    "### 2. forward( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1c98b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "forward: Callable[..., Any] = _forward_unimplemented\n",
    "\n",
    "def _forward_unimplemented(self, *input: Any) -> None:\n",
    "    r\"\"\"Defines the computation performed at every call.\n",
    "\n",
    "    Should be overridden by all subclasses.\n",
    "\n",
    "    .. note::\n",
    "        Although the recipe for forward pass needs to be defined within\n",
    "        this function, one should call the :class:`Module` instance afterwards\n",
    "        instead of this since the former takes care of running the\n",
    "        registered hooks while the latter silently ignores them.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8226c6",
   "metadata": {},
   "source": [
    "\\_\\_init\\_\\_과 더불어 PyTorch에서 반드시 forward는 모델의 계산을 정의한다. (backward 계산은 backward()를 이용하여 알아서 수행해준다)  \n",
    "forward을 override 하지 않을 시 \\_forward_unimplemented method가 NotImplementedError 에러를 일으킨다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08df02e",
   "metadata": {},
   "source": [
    "### 3. register_buffer( ), register_parameter( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "87d4a5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_buffer(self, name: str, tensor: Optional[Tensor], persistent: bool = True) -> None:      \n",
    "        if persistent is False and isinstance(self, torch.jit.ScriptModule):\n",
    "            raise RuntimeError(\"ScriptModule does not support non-persistent buffers\")\n",
    "\n",
    "        if '_buffers' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign buffer before Module.__init__() call\")\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"buffer name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"buffer name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"buffer name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._buffers:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "        elif tensor is not None and not isinstance(tensor, torch.Tensor):\n",
    "            raise TypeError(\"cannot assign '{}' object to buffer '{}' \"\n",
    "                            \"(torch Tensor or None required)\"\n",
    "                            .format(torch.typename(tensor), name))\n",
    "        else:\n",
    "            self._buffers[name] = tensor\n",
    "            if persistent:\n",
    "                self._non_persistent_buffers_set.discard(name)\n",
    "            else:\n",
    "                self._non_persistent_buffers_set.add(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aaff68",
   "metadata": {},
   "source": [
    "- 모듈에 buffer를 추가한다.\n",
    "- 일반적으로 model parameter로 고려 되어서는 안되는 buffer를 등록하는데 사용한다.\n",
    "- 예를들어 BatchNorm의 'running_mean'은 parameter는 아니지만 Module의 일부이다.\n",
    "- persistent를는 buffer를 Model의 일부로 영구적 or 비영구적으로 가져갈 것인지 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "388a2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_parameter(self, name: str, param: Optional[Parameter]) -> None:        \n",
    "        if '_parameters' not in self.__dict__:\n",
    "            raise AttributeError(\n",
    "                \"cannot assign parameter before Module.__init__() call\")\n",
    "\n",
    "        elif not isinstance(name, torch._six.string_classes):\n",
    "            raise TypeError(\"parameter name should be a string. \"\n",
    "                            \"Got {}\".format(torch.typename(name)))\n",
    "        elif '.' in name:\n",
    "            raise KeyError(\"parameter name can't contain \\\".\\\"\")\n",
    "        elif name == '':\n",
    "            raise KeyError(\"parameter name can't be empty string \\\"\\\"\")\n",
    "        elif hasattr(self, name) and name not in self._parameters:\n",
    "            raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "\n",
    "        if param is None:\n",
    "            self._parameters[name] = None\n",
    "        elif not isinstance(param, Parameter):\n",
    "            raise TypeError(\"cannot assign '{}' object to parameter '{}' \"\n",
    "                            \"(torch.nn.Parameter or None required)\"\n",
    "                            .format(torch.typename(param), name))\n",
    "        elif param.grad_fn:\n",
    "            raise ValueError(\n",
    "                \"Cannot assign non-leaf Tensor to parameter '{0}'. Model \"\n",
    "                \"parameters must be created explicitly. To express '{0}' \"\n",
    "                \"as a function of another Tensor, compute the value in \"\n",
    "                \"the forward() method.\".format(name))\n",
    "        else:\n",
    "            self._parameters[name] = param"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947e2d7d",
   "metadata": {},
   "source": [
    "- 모듈에 parameter를 추가한다.\n",
    "- 주어진 name을 사용해 attribute로 access 할 수 있다.\n",
    "- name : parameter의 이름\n",
    "- param : 모듈에 추가할 parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afece7e5",
   "metadata": {},
   "source": [
    "### 4. add_module( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c2eae9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_module(self, name: str, module: Optional['Module']) -> None:\n",
    "    if not isinstance(module, Module) and module is not None:\n",
    "        raise TypeError(\"{} is not a Module subclass\".format(\n",
    "            torch.typename(module)))\n",
    "    elif not isinstance(name, torch._six.string_classes):\n",
    "        raise TypeError(\"module name should be a string. Got {}\".format(\n",
    "            torch.typename(name)))\n",
    "    elif hasattr(self, name) and name not in self._modules:\n",
    "        raise KeyError(\"attribute '{}' already exists\".format(name))\n",
    "    elif '.' in name:\n",
    "        raise KeyError(\"module name can't contain \\\".\\\", got: {}\".format(name))\n",
    "    elif name == '':\n",
    "        raise KeyError(\"module name can't be empty string \\\"\\\"\")\n",
    "    self._modules[name] = module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81a3458",
   "metadata": {},
   "source": [
    "- 현재 Module에 새로운 Module을 추가한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac3e44",
   "metadata": {},
   "source": [
    "### 5. get_submodule( ), get_parameter( ), get_buffer( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "001c8bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def get_submodule(self, target: str) -> \"Module\":\n",
    "        if target == \"\":\n",
    "            return self\n",
    "\n",
    "        atoms: List[str] = target.split(\".\")\n",
    "        mod: torch.nn.Module = self\n",
    "\n",
    "        for item in atoms:\n",
    "\n",
    "            if not hasattr(mod, item):\n",
    "                raise AttributeError(mod._get_name() + \" has no \"\n",
    "                                     \"attribute `\" + item + \"`\")\n",
    "\n",
    "            mod = getattr(mod, item)\n",
    "\n",
    "            if not isinstance(mod, torch.nn.Module):\n",
    "                raise AttributeError(\"`\" + item + \"` is not \"\n",
    "                                     \"an nn.Module\")\n",
    "\n",
    "        return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a26c3",
   "metadata": {},
   "source": [
    "- target 인자로 주어진 submodule을 반환한다.\n",
    "- 없다면 AttributeError를 발생시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9fba08c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameter(self, target: str) -> \"Parameter\":\n",
    "        module_path, _, param_name = target.rpartition(\".\")\n",
    "\n",
    "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
    "\n",
    "        if not hasattr(mod, param_name):\n",
    "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
    "                                 + param_name + \"`\")\n",
    "\n",
    "        param: torch.nn.Parameter = getattr(mod, param_name)\n",
    "\n",
    "        if not isinstance(param, torch.nn.Parameter):\n",
    "            raise AttributeError(\"`\" + param_name + \"` is not an \"\n",
    "                                 \"nn.Parameter\")\n",
    "\n",
    "        return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e41097c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buffer(self, target: str) -> \"Tensor\":        \n",
    "        module_path, _, buffer_name = target.rpartition(\".\")\n",
    "\n",
    "        mod: torch.nn.Module = self.get_submodule(module_path)\n",
    "\n",
    "        if not hasattr(mod, buffer_name):\n",
    "            raise AttributeError(mod._get_name() + \" has no attribute `\"\n",
    "                                 + buffer_name + \"`\")\n",
    "\n",
    "        buffer: torch.Tensor = getattr(mod, buffer_name)\n",
    "\n",
    "        if buffer_name not in mod._buffers:\n",
    "            raise AttributeError(\"`\" + buffer_name + \"` is not a buffer\")\n",
    "\n",
    "        return buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a1b3c0",
   "metadata": {},
   "source": [
    "- target 인자로 주어진 buffer를 반환한다\n",
    "- 경로가 잘못되거나 buffer아 아니면 AttributeError를 일으킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d952d8d",
   "metadata": {},
   "source": [
    "### 6. apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "95fd5e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = TypeVar('T', bound='Module')\n",
    "\n",
    "def apply(self: T, fn: Callable[['Module'], None]) -> T:\n",
    "    for module in self.children():\n",
    "        module.apply(fn)\n",
    "    fn(self)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2bbcdf",
   "metadata": {},
   "source": [
    "- 현재 module의 모든 childern에 fn(추가하고싶은 함수)을 추가한다. \n",
    "- model parameter를 초기화할 때 자주 쓴다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63ab22f",
   "metadata": {},
   "source": [
    "### 7. register_backward_hook( ), register_forward_pre_hook( ), register_forward_hook( )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f507f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_global_backward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "_global_is_full_backward_hook: Optional[bool] = None\n",
    "_global_forward_pre_hooks: Dict[int, Callable] = OrderedDict()\n",
    "_global_forward_hooks: Dict[int, Callable] = OrderedDict()\n",
    "    \n",
    "_grad_t = Union[Tuple[Tensor, ...], Tensor]    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa96f99",
   "metadata": {},
   "source": [
    "추후에 모든 모듈에 공통되는 hook를 추적하기 위해 전역 상태로 만든다(?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "00e1373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_forward_pre_hook(self, hook: Callable[..., None]) -> RemovableHandle:\n",
    "        handle = hooks.RemovableHandle(self._forward_pre_hooks)\n",
    "        self._forward_pre_hooks[handle.id] = hook\n",
    "        return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a3038833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def register_forward_hook(self, hook: Callable[..., None]) -> RemovableHandle:\n",
    "        handle = hooks.RemovableHandle(self._forward_hooks)\n",
    "        self._forward_hooks[handle.id] = hook\n",
    "        return handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d754a6d",
   "metadata": {},
   "source": [
    "#### register_forward_pre_hook \n",
    "- self.\\_forward\\_pre\\_hooks에 hook을 저장한다.\n",
    "- forward_pre_hook은 __forward가 호출 되기전 hook이 호출__ 된다\n",
    "- input을 수정이 할 수 있다.\n",
    "- 반환 할때는 tuple로 warp 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4015da",
   "metadata": {},
   "source": [
    "####  register_forward_hook\n",
    "- self.\\_forward\\_hooks에 hook을 저장한다.\n",
    "- forward_hook은 __forward가 계산을 끝낸 후 hook이 호출__ 된다.\n",
    "- output을 수정 가능하다.(input도 수정이 가능하다 이미 output이 나와있으므로 forward에 영향을 미치지않는다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "7c0dcf60",
   "metadata": {},
   "outputs": [],
   "source": [
    " def register_full_backward_hook(\n",
    "        self, hook: Callable[['Module', _grad_t, _grad_t], Union[None, Tensor]]\n",
    "    ) -> RemovableHandle:      \n",
    "        if self._is_full_backward_hook is False:\n",
    "            raise RuntimeError(\"Cannot use both regular backward hooks and full backward hooks on a \"\n",
    "                               \"single Module. Please use only one of them.\")\n",
    "\n",
    "        self._is_full_backward_hook = True\n",
    "\n",
    "        handle = hooks.RemovableHandle(self._backward_hooks)\n",
    "        self._backward_hooks[handle.id] = hook\n",
    "        return handle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2e1f23",
   "metadata": {},
   "source": [
    "####  register_full_backward_hook\n",
    "- 이미 self.\\_is_full_backward_hook 상태이면 Error를 반환하고 아니면 True로 변경한다\n",
    "- self_backward_hooks에 hook을 저장한다.\n",
    "- Module에 모든 input에 대한 gradient 생성시마다 hook가 호출된다\n",
    "- grad_input과 grad_output은 gradient를 포함하는 튜플이다.\n",
    "- hook는 argument를 수정해서는 안 되지만 grad_input 대신에 사용될 입력과 관련하여 선택적으로 새 gradient를 반환할 수 있다.\n",
    "- RemovableHandle는 추가된 hook를 삭제하는 기능을 해준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c7975a",
   "metadata": {},
   "source": [
    "### 8. \\_\\_call\\_\\_ ,\\_call_impl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3c3633a4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 53)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m53\u001b[0m\n\u001b[0;31m    __call__ : Callable[..., Any] = _call_impl\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def _call_impl(self, *input, **kwargs):\n",
    "        forward_call = (self._slow_forward if torch._C._get_tracing_state() else self.forward)\n",
    "        # hook가 없다면 이부분은 건너 띄고 forward만호출한다\n",
    "        # If we don't have any hooks, we want to skip the rest of the logic in\n",
    "        # this function, and just call forward.\n",
    "        if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
    "                or _global_forward_hooks or _global_forward_pre_hooks):\n",
    "            return forward_call(*input, **kwargs)\n",
    "        # Do not call functions when jit is used\n",
    "        full_backward_hooks, non_full_backward_hooks = [], []\n",
    "        if self._backward_hooks or _global_backward_hooks:\n",
    "            full_backward_hooks, non_full_backward_hooks = self._get_backward_hooks()\n",
    "        if _global_forward_pre_hooks or self._forward_pre_hooks:\n",
    "            for hook in (*_global_forward_pre_hooks.values(), *self._forward_pre_hooks.values()):\n",
    "                result = hook(self, input)\n",
    "                if result is not None:\n",
    "                    if not isinstance(result, tuple):\n",
    "                        result = (result,)\n",
    "                    input = result\n",
    "\n",
    "        bw_hook = None\n",
    "        if full_backward_hooks:\n",
    "            bw_hook = hooks.BackwardHook(self, full_backward_hooks)\n",
    "            input = bw_hook.setup_input_hook(input)\n",
    "\n",
    "        result = forward_call(*input, **kwargs)\n",
    "        if _global_forward_hooks or self._forward_hooks:\n",
    "            for hook in (*_global_forward_hooks.values(), *self._forward_hooks.values()):\n",
    "                hook_result = hook(self, input, result)\n",
    "                if hook_result is not None:\n",
    "                    result = hook_result\n",
    "\n",
    "        if bw_hook:\n",
    "            result = bw_hook.setup_output_hook(result)\n",
    "\n",
    "        # Handle the non-full backward hooks\n",
    "        if non_full_backward_hooks:\n",
    "            var = result\n",
    "            while not isinstance(var, torch.Tensor):\n",
    "                if isinstance(var, dict):\n",
    "                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n",
    "                else:\n",
    "                    var = var[0]\n",
    "            grad_fn = var.grad_fn\n",
    "            if grad_fn is not None:\n",
    "                for hook in non_full_backward_hooks:\n",
    "                    wrapper = functools.partial(hook, self)\n",
    "                    functools.update_wrapper(wrapper, hook)\n",
    "                    grad_fn.register_hook(wrapper)\n",
    "                self._maybe_warn_non_full_backward_hook(input, result, grad_fn)\n",
    "\n",
    "        return result\n",
    "\n",
    "    __call__ : Callable[..., Any] = _call_impl\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048b8c17",
   "metadata": {},
   "source": [
    "- \\_\\_call\\_\\_은 인스턴스가 호출될때 사용되는 Python의 magic method이다.\n",
    "- PyTorch에서는 \\_call_impl로 오버라이딩(?) 하였다.\n",
    "- <code> if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n",
    "or _global_forward_hooks or _global_forward_pre_hooks)...</code> 에서  보다시피 hook이 없다면 건너 띄고 forward를 수행하게 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58df51fc",
   "metadata": {},
   "source": [
    "### 9. train( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dea7339c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self: T, mode: bool = True) -> T:\n",
    "    r\"\"\"Sets the module in training mode.\n",
    "\n",
    "    This has any effect only on certain modules. See documentations of\n",
    "    particular modules for details of their behaviors in training/evaluation\n",
    "    mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
    "    etc.\n",
    "\n",
    "    Args:\n",
    "        mode (bool): whether to set training mode (``True``) or evaluation\n",
    "                     mode (``False``). Default: ``True``.\n",
    "\n",
    "    Returns:\n",
    "        Module: self\n",
    "    \"\"\"\n",
    "    if not isinstance(mode, bool):\n",
    "        raise ValueError(\"training mode is expected to be boolean\")\n",
    "    self.training = mode\n",
    "    for module in self.children():\n",
    "        module.train(mode)\n",
    "    return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f65c296",
   "metadata": {},
   "source": [
    "- True일시, Module을 training mode로 설정한다\n",
    "- init에서 True로 설정한 self.training값을 변경한다\n",
    "- 모든 Module에 영향을 미치는 것은 아니라고 한다.(영향을 받는 경우 : Dropout, BatchNorm등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34deaa02",
   "metadata": {},
   "source": [
    "### 10. zero_grad( )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3fb282d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, set_to_none: bool = False) -> None:\n",
    "        if getattr(self, '_is_replica', False):\n",
    "            warnings.warn(\n",
    "                \"Calling .zero_grad() from a module created with nn.DataParallel() has no effect. \"\n",
    "                \"The parameters are copied (in a differentiable manner) from the original module. \"\n",
    "                \"This means they are not leaf nodes in autograd and so don't accumulate gradients. \"\n",
    "                \"If you need gradients in your forward method, consider using autograd.grad instead.\")\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.grad is not None:\n",
    "                if set_to_none:\n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad.detach_()\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35044b1",
   "metadata": {},
   "source": [
    "- 모든 Model의 parameters를 0으로 한다\n",
    "- torch.optim.Optimizer와 유사하다.\n",
    "- set_to_none 는 grads를 None으로 변경할 수 있는 parameter이다.(torch.optim.Optimizer.zero_grad에서 더 자세히 확인가능)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6cc0e",
   "metadata": {},
   "source": [
    "### 11. extra_repr( ), \\_\\_repr\\_\\_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0ae8250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extra_repr(self) -> str:\n",
    "    \n",
    "    return ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725aed78",
   "metadata": {},
   "source": [
    "- extra_repr은 PyTorch에서 repr을 표현하는 데 사용하는 method이다.\n",
    "- 빈문자열인 이유는 각 사용자가 자신이 만든 Module 에 맞는 설명을 apply 함수로 채워 넣으라는 의미인거 같다.\n",
    "- 그리고 직접적으로 출력을 해주는 것은 아래 \\_\\_repr\\_\\_ method 를 거쳐야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a6c2f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __repr__(self):\n",
    "        # We treat the extra repr like the sub-module, one item per line\n",
    "        extra_lines = []\n",
    "        extra_repr = self.extra_repr()\n",
    "        # empty string will be split into list ['']\n",
    "        if extra_repr:\n",
    "            extra_lines = extra_repr.split('\\n')\n",
    "        child_lines = []\n",
    "        for key, module in self._modules.items():\n",
    "            mod_str = repr(module)\n",
    "            mod_str = _addindent(mod_str, 2)\n",
    "            child_lines.append('(' + key + '): ' + mod_str)\n",
    "        lines = extra_lines + child_lines\n",
    "\n",
    "        main_str = self._get_name() + '('\n",
    "        if lines:\n",
    "            # simple one-liner info, which most builtin Modules will use\n",
    "            if len(extra_lines) == 1 and not child_lines:\n",
    "                main_str += extra_lines[0]\n",
    "            else:\n",
    "                main_str += '\\n  ' + '\\n  '.join(lines) + '\\n'\n",
    "\n",
    "        main_str += ')'\n",
    "        return main_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b14ee9",
   "metadata": {},
   "source": [
    "- \\_\\_repr\\_\\_ 은 Python의 Magic Method 중 하나이다. str과도 비슷하지만 str은 있는 그대로의 문자열을 반환해주는 역할이라면 repr은 좀 더 사용자에의 이해를 돕는 방향으로 객체를 표현해준다.\n",
    "- \\_\\_repr\\_\\_의 역할은 간단하다. self.extra_repr( )을 가져와서 출력해주는 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52a589",
   "metadata": {},
   "source": [
    "### 12. 그 외 메서드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11776f41",
   "metadata": {},
   "source": [
    "- parameters\n",
    "- named_parameters\n",
    "- buffers\n",
    "- named_buffers\n",
    "- children\n",
    "- named_children\n",
    "- modules\n",
    "- named_modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9b9e76",
   "metadata": {},
   "source": [
    "각각 해당 모듈의 parameters, buffers, childern, modules를 보여준다는 특징이 있다.  \n",
    "앞에 named_ 가 붙으면 iteration에서 name, module로 분리되어 나오기 때문에 원하는 name에 해당하는 module을 골라낼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
