{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e89a3f55",
   "metadata": {},
   "source": [
    "## torch.utils.data\n",
    "여기서는 크게 3가지를 알면된다.\n",
    "- Sampler\n",
    "- Dataset\n",
    "- Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbf963f",
   "metadata": {},
   "source": [
    "### Samlpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14c28bd8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Generic' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3b/rtm0n0l56yd2mbcz2vh1106r0000gn/T/ipykernel_12607/3189102701.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGeneric\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT_co\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     r\"\"\"Base class for all Samplers.\n\u001b[1;32m      3\u001b[0m     \u001b[0mEvery\u001b[0m \u001b[0mSampler\u001b[0m \u001b[0msubclass\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mto\u001b[0m \u001b[0mprovide\u001b[0m \u001b[0man\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproviding\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mway\u001b[0m \u001b[0mto\u001b[0m \u001b[0miterate\u001b[0m \u001b[0mover\u001b[0m \u001b[0mindices\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0melements\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0m__len__\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mthat\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlength\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreturned\u001b[0m \u001b[0miterators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Generic' is not defined"
     ]
    }
   ],
   "source": [
    "class Sampler(Generic[T_co]):\n",
    "    r\"\"\"Base class for all Samplers.\n",
    "    Every Sampler subclass has to provide an :meth:`__iter__` method, providing a\n",
    "    way to iterate over indices of dataset elements, and a :meth:`__len__` method\n",
    "    that returns the length of the returned iterators.\n",
    "    .. note:: The :meth:`__len__` method isn't strictly required by\n",
    "              :class:`~torch.utils.data.DataLoader`, but is expected in any\n",
    "              calculation involving the length of a :class:`~torch.utils.data.DataLoader`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_source: Optional[Sized]) -> None:\n",
    "        pass\n",
    "\n",
    "    def __iter__(self) -> Iterator[T_co]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "    #\n",
    "    # Many times we have an abstract class representing a collection/iterable of\n",
    "    # data, e.g., `torch.utils.data.Sampler`, with its subclasses optionally\n",
    "    # implementing a `__len__` method. In such cases, we must make sure to not\n",
    "    # provide a default implementation, because both straightforward default\n",
    "    # implementations have their issues:\n",
    "    #\n",
    "    #   + `return NotImplemented`:\n",
    "    #     Calling `len(subclass_instance)` raises:\n",
    "    #       TypeError: 'NotImplementedType' object cannot be interpreted as an integer\n",
    "    #\n",
    "    #   + `raise NotImplementedError()`:\n",
    "    #     This prevents triggering some fallback behavior. E.g., the built-in\n",
    "    #     `list(X)` tries to call `len(X)` first, and executes a different code\n",
    "    #     path if the method is not found or `NotImplemented` is returned, while\n",
    "    #     raising an `NotImplementedError` will propagate and and make the call\n",
    "    #     fail where it could have use `__iter__` to complete the call.\n",
    "    #\n",
    "    # Thus, the only two sensible things to do are\n",
    "    #\n",
    "    #   + **not** provide a default `__len__`.\n",
    "    #\n",
    "    #   + raise a `TypeError` instead, which is what Python uses when users call\n",
    "    #     a method that is not defined on an object.\n",
    "    #     (@ssnl verifies that this works on at least Python 3.7.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a203797",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d5735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(Generic[T_co]):\n",
    "    def __getitem__(self, index) -> T_co:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __add__(self, other: 'Dataset[T_co]') -> 'ConcatDataset[T_co]':\n",
    "        return ConcatDataset([self, other])\n",
    "\n",
    "    # No `def __len__(self)` default?\n",
    "    # See NOTE [ Lack of Default `__len__` in Python Abstract Base Classes ]\n",
    "    # in pytorch/torch/utils/data/sampler.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4d6e",
   "metadata": {},
   "source": [
    "- 모든 datasets 는 이 class의 subclass 여야한다(?)\n",
    "- 모든 subclass들은 \\_\\_get\\_\\_이 있어야한다.\n",
    "- 선택적으로 size를 return 해주는 \\_\\_len\\_\\_을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e8adf",
   "metadata": {},
   "source": [
    "### Samlpler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61388d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Generic[T_co]):\n",
    "    r\"\"\"\n",
    "    Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
    "    the given dataset.\n",
    "    The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
    "    iterable-style datasets with single- or multi-process loading, customizing\n",
    "    loading order and optional automatic batching (collation) and memory pinning.\n",
    "    See :py:mod:`torch.utils.data` documentation page for more details.\n",
    "    Args:\n",
    "        dataset (Dataset): dataset from which to load the data.\n",
    "        batch_size (int, optional): how many samples per batch to load\n",
    "            (default: ``1``).\n",
    "        shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
    "            at every epoch (default: ``False``).\n",
    "        sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
    "            samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
    "            implemented. If specified, :attr:`shuffle` must not be specified.\n",
    "        batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
    "            returns a batch of indices at a time. Mutually exclusive with\n",
    "            :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
    "            and :attr:`drop_last`.\n",
    "        num_workers (int, optional): how many subprocesses to use for data\n",
    "            loading. ``0`` means that the data will be loaded in the main process.\n",
    "            (default: ``0``)\n",
    "        collate_fn (callable, optional): merges a list of samples to form a\n",
    "            mini-batch of Tensor(s).  Used when using batched loading from a\n",
    "            map-style dataset.\n",
    "        pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
    "            into CUDA pinned memory before returning them.  If your data elements\n",
    "            are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
    "            see the example below.\n",
    "        drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
    "            if the dataset size is not divisible by the batch size. If ``False`` and\n",
    "            the size of dataset is not divisible by the batch size, then the last batch\n",
    "            will be smaller. (default: ``False``)\n",
    "        timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
    "            from workers. Should always be non-negative. (default: ``0``)\n",
    "        worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
    "            worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
    "            input, after seeding and before data loading. (default: ``None``)\n",
    "        generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
    "            by RandomSampler to generate random indexes and multiprocessing to generate\n",
    "            `base_seed` for workers. (default: ``None``)\n",
    "        prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
    "            in advance by each worker. ``2`` means there will be a total of\n",
    "            2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
    "        persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
    "            the worker processes after a dataset has been consumed once. This allows to\n",
    "            maintain the workers `Dataset` instances alive. (default: ``False``)\n",
    "    .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
    "                 cannot be an unpicklable object, e.g., a lambda function. See\n",
    "                 :ref:`multiprocessing-best-practices` on more details related\n",
    "                 to multiprocessing in PyTorch.\n",
    "    .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
    "                 When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
    "                 it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
    "                 rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
    "                 configurations. This represents the best guess PyTorch can make because PyTorch\n",
    "                 trusts user :attr:`dataset` code in correctly handling multi-process\n",
    "                 loading to avoid duplicate data.\n",
    "                 However, if sharding results in multiple workers having incomplete last batches,\n",
    "                 this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
    "                 be broken into multiple ones and (2) more than one batch worth of samples can be\n",
    "                 dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
    "                 cases in general.\n",
    "                 See `Dataset Types`_ for more details on these two types of datasets and how\n",
    "                 :class:`~torch.utils.data.IterableDataset` interacts with\n",
    "                 `Multi-process data loading`_.\n",
    "    .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
    "                 :ref:`data-loading-randomness` notes for random seed related questions.\n",
    "    \"\"\"\n",
    "    dataset: Dataset[T_co]\n",
    "    batch_size: Optional[int]\n",
    "    num_workers: int\n",
    "    pin_memory: bool\n",
    "    drop_last: bool\n",
    "    timeout: float\n",
    "    sampler: Union[Sampler, Iterable]\n",
    "    prefetch_factor: int\n",
    "    _iterator : Optional['_BaseDataLoaderIter']\n",
    "    __initialized = False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
