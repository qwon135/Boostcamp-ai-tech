{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43d3c2a",
   "metadata": {},
   "source": [
    "#  AutoGrad & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95044ed7",
   "metadata": {},
   "source": [
    "논문 구현 : 수 많은 반복의 연속  \n",
    "Layer = Block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007d2e6",
   "metadata": {},
   "source": [
    "예시) Transformer\n",
    "- softmax\n",
    "- Linear(xW+b)\n",
    "- Normalization\n",
    "- Multi-Head Attension\n",
    "- Layer를 훔쳐서 큰 block을 만드는 Encoder, Decoder  \n",
    "\n",
    "즉 Layer를 쌓는것은 블록의 연속"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c27e3bf",
   "metadata": {},
   "source": [
    "이 것을 만들기 위한 기본이 Pytorch의 nn.moudle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ccba82",
   "metadata": {},
   "source": [
    "### torch.nn.Module\n",
    "- 딥러닝을 구성하는 Layer의 base class(Auto Grad)\n",
    "- 4가지를 정함 : Input, Output, Forward(이 때 무슨일이 일어나는가), Backward(weight)\n",
    "- 학습의 대상이 되는 parameter(tensor) 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf6b808",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_33.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7670511",
   "metadata": {},
   "source": [
    "가장 일반적인 nn.module\n",
    "- Forwardpass : input(x,y)이 있고 output(z)이 있다.  \n",
    "- Backwardpass : 미분 값을 넣어 각각의 미분값 뱉음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b272df",
   "metadata": {},
   "source": [
    "### nn.Parameter\n",
    "- Tensor 객체의 상속 객체\n",
    "- nn.Module 내에서 __attribute__ 가 될 때는 required_grad=True(Auto Grad)로 지정되어 학습 대상이 되는 Tensor\n",
    "- 직접 지정할 일은 잘 없다 : 대부분의 layer에는 weights 값들이 지정되어 잇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a2fe4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "\n",
    "# 예시(torch에는 이미 Linear가 구현되어 있다)\n",
    "class MyLiner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features \n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = nn.Parameter(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bc4a61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a186c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c9b9ea7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 12])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = MyLiner(7, 12) # x의 크기를 변경해주는 class MyLiner\n",
    "layer(x).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7e92fa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-1.4592e-01,  5.0993e-01, -2.3244e-01, -7.3565e-02,  3.9677e-02,\n",
      "          2.2398e-01,  2.0214e-01, -2.5409e+00, -1.9219e+00, -1.7530e-01,\n",
      "          6.3914e-01, -1.6434e+00],\n",
      "        [ 1.7428e-01,  1.0911e+00,  6.3016e-01,  2.4145e+00,  8.7708e-01,\n",
      "         -3.5512e-01,  1.1642e+00, -1.4017e+00,  1.4199e-01,  1.3938e+00,\n",
      "         -5.7261e-01, -8.3358e-01],\n",
      "        [ 3.7805e-01,  8.3586e-01, -1.3863e+00, -4.6463e-01,  9.0152e-01,\n",
      "          8.4283e-01, -2.1672e-01,  1.2465e+00, -7.9584e-03, -6.8982e-01,\n",
      "         -1.6935e-01, -1.0023e+00],\n",
      "        [ 3.6371e-01, -1.4043e+00, -3.1261e-01, -1.8607e-01, -1.0783e+00,\n",
      "          1.4753e+00, -2.3888e+00,  4.6925e-01, -1.7411e+00, -3.6349e-02,\n",
      "         -8.7827e-01,  6.0790e-01],\n",
      "        [ 1.1703e+00, -8.2422e-01,  1.3809e+00,  5.0822e-01,  5.9645e-01,\n",
      "          5.3550e-01, -6.7792e-01,  1.2961e+00,  1.7554e+00,  9.6315e-01,\n",
      "          3.2037e-02, -2.0908e+00],\n",
      "        [ 4.6476e-01, -6.7218e-01,  9.8372e-01, -5.1118e-01,  6.0685e-01,\n",
      "         -2.2032e-01,  7.5679e-01, -2.9582e-03,  2.4424e-01, -6.1929e-01,\n",
      "         -1.8362e-01,  6.2724e-01],\n",
      "        [-4.0542e+00,  9.0800e-02,  2.9894e-01,  7.8629e-01,  1.9047e+00,\n",
      "         -6.7897e-01, -2.0808e+00, -1.2187e+00, -6.9671e-02, -1.1782e+00,\n",
      "          3.3420e-01,  1.0974e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.4284,  0.7161, -0.6244,  1.2879,  0.4432,  0.3783,  0.4099, -0.9521,\n",
      "        -1.2373, -2.3275,  0.6190,  1.4668], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for value in layer.parameters(): # 미분의 대상을 보여준다.\n",
    "    print(value) # backward가 일어날 때 autograd가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e6e05",
   "metadata": {},
   "source": [
    "### Backward\n",
    "- Layer에 있는 Parameter 들의 미분을 수행\n",
    "- Forward의 결과값(model의 __output=예측치__)과 실제값간의 loss에 대해 미분\n",
    "- 해당 값으로 Parameter 업데이트\n",
    "- 총 4단계\n",
    " 1. optimizer.zero_grad() \n",
    " 2. loss = criterion(outputs, labels)\n",
    " 3. loss.backward()\n",
    " 4. optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921640c8",
   "metadata": {},
   "source": [
    "$$y = 2x + 1  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "57369c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "# train data 생성\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype = np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype = np.float32)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61edee7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2669ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6d99986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1 \n",
    "outputDim = 1\n",
    "learningRate = 0.01\n",
    "epochs = 1000\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7680ac37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(250.6362, grad_fn=<MseLossBackward0>)\n",
      "0 250.6361846923828\n",
      "tensor(20.5625, grad_fn=<MseLossBackward0>)\n",
      "1 20.5625\n",
      "tensor(1.7948, grad_fn=<MseLossBackward0>)\n",
      "2 1.7947888374328613\n",
      "tensor(0.2627, grad_fn=<MseLossBackward0>)\n",
      "3 0.2626533508300781\n",
      "tensor(0.1364, grad_fn=<MseLossBackward0>)\n",
      "4 0.13638392090797424\n",
      "tensor(0.1248, grad_fn=<MseLossBackward0>)\n",
      "5 0.12480084598064423\n",
      "tensor(0.1226, grad_fn=<MseLossBackward0>)\n",
      "6 0.12258653342723846\n",
      "tensor(0.1212, grad_fn=<MseLossBackward0>)\n",
      "7 0.12115068733692169\n",
      "tensor(0.1198, grad_fn=<MseLossBackward0>)\n",
      "8 0.11979236453771591\n",
      "tensor(0.1185, grad_fn=<MseLossBackward0>)\n",
      "9 0.1184542253613472\n",
      "tensor(0.1171, grad_fn=<MseLossBackward0>)\n",
      "10 0.11713140457868576\n",
      "tensor(0.1158, grad_fn=<MseLossBackward0>)\n",
      "11 0.11582336574792862\n",
      "tensor(0.1145, grad_fn=<MseLossBackward0>)\n",
      "12 0.11452999711036682\n",
      "tensor(0.1133, grad_fn=<MseLossBackward0>)\n",
      "13 0.11325094848871231\n",
      "tensor(0.1120, grad_fn=<MseLossBackward0>)\n",
      "14 0.11198630928993225\n",
      "tensor(0.1107, grad_fn=<MseLossBackward0>)\n",
      "15 0.11073587089776993\n",
      "tensor(0.1095, grad_fn=<MseLossBackward0>)\n",
      "16 0.10949921607971191\n",
      "tensor(0.1083, grad_fn=<MseLossBackward0>)\n",
      "17 0.10827671736478806\n",
      "tensor(0.1071, grad_fn=<MseLossBackward0>)\n",
      "18 0.10706760734319687\n",
      "tensor(0.1059, grad_fn=<MseLossBackward0>)\n",
      "19 0.10587172210216522\n",
      "tensor(0.1047, grad_fn=<MseLossBackward0>)\n",
      "20 0.1046895906329155\n",
      "tensor(0.1035, grad_fn=<MseLossBackward0>)\n",
      "21 0.10352058708667755\n",
      "tensor(0.1024, grad_fn=<MseLossBackward0>)\n",
      "22 0.10236451029777527\n",
      "tensor(0.1012, grad_fn=<MseLossBackward0>)\n",
      "23 0.1012214720249176\n",
      "tensor(0.1001, grad_fn=<MseLossBackward0>)\n",
      "24 0.1000911220908165\n",
      "tensor(0.0990, grad_fn=<MseLossBackward0>)\n",
      "25 0.09897331893444061\n",
      "tensor(0.0979, grad_fn=<MseLossBackward0>)\n",
      "26 0.09786822646856308\n",
      "tensor(0.0968, grad_fn=<MseLossBackward0>)\n",
      "27 0.0967753455042839\n",
      "tensor(0.0957, grad_fn=<MseLossBackward0>)\n",
      "28 0.0956946313381195\n",
      "tensor(0.0946, grad_fn=<MseLossBackward0>)\n",
      "29 0.0946260467171669\n",
      "tensor(0.0936, grad_fn=<MseLossBackward0>)\n",
      "30 0.0935693010687828\n",
      "tensor(0.0925, grad_fn=<MseLossBackward0>)\n",
      "31 0.09252443164587021\n",
      "tensor(0.0915, grad_fn=<MseLossBackward0>)\n",
      "32 0.09149120002985\n",
      "tensor(0.0905, grad_fn=<MseLossBackward0>)\n",
      "33 0.09046952426433563\n",
      "tensor(0.0895, grad_fn=<MseLossBackward0>)\n",
      "34 0.0894593670964241\n",
      "tensor(0.0885, grad_fn=<MseLossBackward0>)\n",
      "35 0.08846044540405273\n",
      "tensor(0.0875, grad_fn=<MseLossBackward0>)\n",
      "36 0.08747261762619019\n",
      "tensor(0.0865, grad_fn=<MseLossBackward0>)\n",
      "37 0.08649574965238571\n",
      "tensor(0.0855, grad_fn=<MseLossBackward0>)\n",
      "38 0.08552989363670349\n",
      "tensor(0.0846, grad_fn=<MseLossBackward0>)\n",
      "39 0.08457475155591965\n",
      "tensor(0.0836, grad_fn=<MseLossBackward0>)\n",
      "40 0.08363031595945358\n",
      "tensor(0.0827, grad_fn=<MseLossBackward0>)\n",
      "41 0.08269646018743515\n",
      "tensor(0.0818, grad_fn=<MseLossBackward0>)\n",
      "42 0.08177300542593002\n",
      "tensor(0.0809, grad_fn=<MseLossBackward0>)\n",
      "43 0.08085999637842178\n",
      "tensor(0.0800, grad_fn=<MseLossBackward0>)\n",
      "44 0.07995688170194626\n",
      "tensor(0.0791, grad_fn=<MseLossBackward0>)\n",
      "45 0.07906404137611389\n",
      "tensor(0.0782, grad_fn=<MseLossBackward0>)\n",
      "46 0.07818115502595901\n",
      "tensor(0.0773, grad_fn=<MseLossBackward0>)\n",
      "47 0.07730817049741745\n",
      "tensor(0.0764, grad_fn=<MseLossBackward0>)\n",
      "48 0.07644475251436234\n",
      "tensor(0.0756, grad_fn=<MseLossBackward0>)\n",
      "49 0.07559123635292053\n",
      "tensor(0.0747, grad_fn=<MseLossBackward0>)\n",
      "50 0.07474697381258011\n",
      "tensor(0.0739, grad_fn=<MseLossBackward0>)\n",
      "51 0.07391241937875748\n",
      "tensor(0.0731, grad_fn=<MseLossBackward0>)\n",
      "52 0.07308705151081085\n",
      "tensor(0.0723, grad_fn=<MseLossBackward0>)\n",
      "53 0.07227083295583725\n",
      "tensor(0.0715, grad_fn=<MseLossBackward0>)\n",
      "54 0.07146389037370682\n",
      "tensor(0.0707, grad_fn=<MseLossBackward0>)\n",
      "55 0.07066585123538971\n",
      "tensor(0.0699, grad_fn=<MseLossBackward0>)\n",
      "56 0.06987661868333817\n",
      "tensor(0.0691, grad_fn=<MseLossBackward0>)\n",
      "57 0.06909636408090591\n",
      "tensor(0.0683, grad_fn=<MseLossBackward0>)\n",
      "58 0.06832491606473923\n",
      "tensor(0.0676, grad_fn=<MseLossBackward0>)\n",
      "59 0.0675617977976799\n",
      "tensor(0.0668, grad_fn=<MseLossBackward0>)\n",
      "60 0.06680727750062943\n",
      "tensor(0.0661, grad_fn=<MseLossBackward0>)\n",
      "61 0.06606137007474899\n",
      "tensor(0.0653, grad_fn=<MseLossBackward0>)\n",
      "62 0.06532365083694458\n",
      "tensor(0.0646, grad_fn=<MseLossBackward0>)\n",
      "63 0.06459426134824753\n",
      "tensor(0.0639, grad_fn=<MseLossBackward0>)\n",
      "64 0.06387285888195038\n",
      "tensor(0.0632, grad_fn=<MseLossBackward0>)\n",
      "65 0.06315965205430984\n",
      "tensor(0.0625, grad_fn=<MseLossBackward0>)\n",
      "66 0.0624542273581028\n",
      "tensor(0.0618, grad_fn=<MseLossBackward0>)\n",
      "67 0.06175697222352028\n",
      "tensor(0.0611, grad_fn=<MseLossBackward0>)\n",
      "68 0.061067208647727966\n",
      "tensor(0.0604, grad_fn=<MseLossBackward0>)\n",
      "69 0.06038526073098183\n",
      "tensor(0.0597, grad_fn=<MseLossBackward0>)\n",
      "70 0.05971100553870201\n",
      "tensor(0.0590, grad_fn=<MseLossBackward0>)\n",
      "71 0.05904427915811539\n",
      "tensor(0.0584, grad_fn=<MseLossBackward0>)\n",
      "72 0.05838494002819061\n",
      "tensor(0.0577, grad_fn=<MseLossBackward0>)\n",
      "73 0.05773293972015381\n",
      "tensor(0.0571, grad_fn=<MseLossBackward0>)\n",
      "74 0.05708817020058632\n",
      "tensor(0.0565, grad_fn=<MseLossBackward0>)\n",
      "75 0.056450698524713516\n",
      "tensor(0.0558, grad_fn=<MseLossBackward0>)\n",
      "76 0.05582033097743988\n",
      "tensor(0.0552, grad_fn=<MseLossBackward0>)\n",
      "77 0.055197056382894516\n",
      "tensor(0.0546, grad_fn=<MseLossBackward0>)\n",
      "78 0.05458061397075653\n",
      "tensor(0.0540, grad_fn=<MseLossBackward0>)\n",
      "79 0.05397113412618637\n",
      "tensor(0.0534, grad_fn=<MseLossBackward0>)\n",
      "80 0.053368572145700455\n",
      "tensor(0.0528, grad_fn=<MseLossBackward0>)\n",
      "81 0.05277246609330177\n",
      "tensor(0.0522, grad_fn=<MseLossBackward0>)\n",
      "82 0.052183255553245544\n",
      "tensor(0.0516, grad_fn=<MseLossBackward0>)\n",
      "83 0.05160049349069595\n",
      "tensor(0.0510, grad_fn=<MseLossBackward0>)\n",
      "84 0.05102428421378136\n",
      "tensor(0.0505, grad_fn=<MseLossBackward0>)\n",
      "85 0.05045448616147041\n",
      "tensor(0.0499, grad_fn=<MseLossBackward0>)\n",
      "86 0.04989100620150566\n",
      "tensor(0.0493, grad_fn=<MseLossBackward0>)\n",
      "87 0.04933389648795128\n",
      "tensor(0.0488, grad_fn=<MseLossBackward0>)\n",
      "88 0.04878300800919533\n",
      "tensor(0.0482, grad_fn=<MseLossBackward0>)\n",
      "89 0.048238348215818405\n",
      "tensor(0.0477, grad_fn=<MseLossBackward0>)\n",
      "90 0.047699570655822754\n",
      "tensor(0.0472, grad_fn=<MseLossBackward0>)\n",
      "91 0.047166939824819565\n",
      "tensor(0.0466, grad_fn=<MseLossBackward0>)\n",
      "92 0.04664028063416481\n",
      "tensor(0.0461, grad_fn=<MseLossBackward0>)\n",
      "93 0.04611949995160103\n",
      "tensor(0.0456, grad_fn=<MseLossBackward0>)\n",
      "94 0.04560438171029091\n",
      "tensor(0.0451, grad_fn=<MseLossBackward0>)\n",
      "95 0.04509516805410385\n",
      "tensor(0.0446, grad_fn=<MseLossBackward0>)\n",
      "96 0.04459153860807419\n",
      "tensor(0.0441, grad_fn=<MseLossBackward0>)\n",
      "97 0.04409352317452431\n",
      "tensor(0.0436, grad_fn=<MseLossBackward0>)\n",
      "98 0.04360126703977585\n",
      "tensor(0.0431, grad_fn=<MseLossBackward0>)\n",
      "99 0.04311427101492882\n",
      "tensor(0.0426, grad_fn=<MseLossBackward0>)\n",
      "100 0.04263298586010933\n",
      "tensor(0.0422, grad_fn=<MseLossBackward0>)\n",
      "101 0.04215673357248306\n",
      "tensor(0.0417, grad_fn=<MseLossBackward0>)\n",
      "102 0.04168601334095001\n",
      "tensor(0.0412, grad_fn=<MseLossBackward0>)\n",
      "103 0.04122060164809227\n",
      "tensor(0.0408, grad_fn=<MseLossBackward0>)\n",
      "104 0.04076022654771805\n",
      "tensor(0.0403, grad_fn=<MseLossBackward0>)\n",
      "105 0.04030512273311615\n",
      "tensor(0.0399, grad_fn=<MseLossBackward0>)\n",
      "106 0.03985500708222389\n",
      "tensor(0.0394, grad_fn=<MseLossBackward0>)\n",
      "107 0.03940994665026665\n",
      "tensor(0.0390, grad_fn=<MseLossBackward0>)\n",
      "108 0.038969919085502625\n",
      "tensor(0.0385, grad_fn=<MseLossBackward0>)\n",
      "109 0.038534753024578094\n",
      "tensor(0.0381, grad_fn=<MseLossBackward0>)\n",
      "110 0.03810438886284828\n",
      "tensor(0.0377, grad_fn=<MseLossBackward0>)\n",
      "111 0.03767886757850647\n",
      "tensor(0.0373, grad_fn=<MseLossBackward0>)\n",
      "112 0.037258122116327286\n",
      "tensor(0.0368, grad_fn=<MseLossBackward0>)\n",
      "113 0.03684210777282715\n",
      "tensor(0.0364, grad_fn=<MseLossBackward0>)\n",
      "114 0.03643063083291054\n",
      "tensor(0.0360, grad_fn=<MseLossBackward0>)\n",
      "115 0.03602378070354462\n",
      "tensor(0.0356, grad_fn=<MseLossBackward0>)\n",
      "116 0.03562149778008461\n",
      "tensor(0.0352, grad_fn=<MseLossBackward0>)\n",
      "117 0.035223811864852905\n",
      "tensor(0.0348, grad_fn=<MseLossBackward0>)\n",
      "118 0.03483041003346443\n",
      "tensor(0.0344, grad_fn=<MseLossBackward0>)\n",
      "119 0.034441474825143814\n",
      "tensor(0.0341, grad_fn=<MseLossBackward0>)\n",
      "120 0.03405681625008583\n",
      "tensor(0.0337, grad_fn=<MseLossBackward0>)\n",
      "121 0.03367659077048302\n",
      "tensor(0.0333, grad_fn=<MseLossBackward0>)\n",
      "122 0.03330042213201523\n",
      "tensor(0.0329, grad_fn=<MseLossBackward0>)\n",
      "123 0.03292860835790634\n",
      "tensor(0.0326, grad_fn=<MseLossBackward0>)\n",
      "124 0.03256092965602875\n",
      "tensor(0.0322, grad_fn=<MseLossBackward0>)\n",
      "125 0.03219732642173767\n",
      "tensor(0.0318, grad_fn=<MseLossBackward0>)\n",
      "126 0.031837768852710724\n",
      "tensor(0.0315, grad_fn=<MseLossBackward0>)\n",
      "127 0.031482256948947906\n",
      "tensor(0.0311, grad_fn=<MseLossBackward0>)\n",
      "128 0.031130628660321236\n",
      "tensor(0.0308, grad_fn=<MseLossBackward0>)\n",
      "129 0.03078305907547474\n",
      "tensor(0.0304, grad_fn=<MseLossBackward0>)\n",
      "130 0.030439330264925957\n",
      "tensor(0.0301, grad_fn=<MseLossBackward0>)\n",
      "131 0.030099427327513695\n",
      "tensor(0.0298, grad_fn=<MseLossBackward0>)\n",
      "132 0.02976338379085064\n",
      "tensor(0.0294, grad_fn=<MseLossBackward0>)\n",
      "133 0.0294309314340353\n",
      "tensor(0.0291, grad_fn=<MseLossBackward0>)\n",
      "134 0.02910231426358223\n",
      "tensor(0.0288, grad_fn=<MseLossBackward0>)\n",
      "135 0.02877732366323471\n",
      "tensor(0.0285, grad_fn=<MseLossBackward0>)\n",
      "136 0.028455881401896477\n",
      "tensor(0.0281, grad_fn=<MseLossBackward0>)\n",
      "137 0.028138162568211555\n",
      "tensor(0.0278, grad_fn=<MseLossBackward0>)\n",
      "138 0.027823960408568382\n",
      "tensor(0.0275, grad_fn=<MseLossBackward0>)\n",
      "139 0.02751324698328972\n",
      "tensor(0.0272, grad_fn=<MseLossBackward0>)\n",
      "140 0.027206022292375565\n",
      "tensor(0.0269, grad_fn=<MseLossBackward0>)\n",
      "141 0.02690226584672928\n",
      "tensor(0.0266, grad_fn=<MseLossBackward0>)\n",
      "142 0.0266017597168684\n",
      "tensor(0.0263, grad_fn=<MseLossBackward0>)\n",
      "143 0.026304740458726883\n",
      "tensor(0.0260, grad_fn=<MseLossBackward0>)\n",
      "144 0.026011014357209206\n",
      "tensor(0.0257, grad_fn=<MseLossBackward0>)\n",
      "145 0.025720490142703056\n",
      "tensor(0.0254, grad_fn=<MseLossBackward0>)\n",
      "146 0.025433333590626717\n",
      "tensor(0.0251, grad_fn=<MseLossBackward0>)\n",
      "147 0.02514934353530407\n",
      "tensor(0.0249, grad_fn=<MseLossBackward0>)\n",
      "148 0.024868501350283623\n",
      "tensor(0.0246, grad_fn=<MseLossBackward0>)\n",
      "149 0.02459081821143627\n",
      "tensor(0.0243, grad_fn=<MseLossBackward0>)\n",
      "150 0.024316193535923958\n",
      "tensor(0.0240, grad_fn=<MseLossBackward0>)\n",
      "151 0.024044619873166084\n",
      "tensor(0.0238, grad_fn=<MseLossBackward0>)\n",
      "152 0.02377617172896862\n",
      "tensor(0.0235, grad_fn=<MseLossBackward0>)\n",
      "153 0.02351059392094612\n",
      "tensor(0.0232, grad_fn=<MseLossBackward0>)\n",
      "154 0.02324807085096836\n",
      "tensor(0.0230, grad_fn=<MseLossBackward0>)\n",
      "155 0.022988557815551758\n",
      "tensor(0.0227, grad_fn=<MseLossBackward0>)\n",
      "156 0.022731786593794823\n",
      "tensor(0.0225, grad_fn=<MseLossBackward0>)\n",
      "157 0.02247791551053524\n",
      "tensor(0.0222, grad_fn=<MseLossBackward0>)\n",
      "158 0.022226858884096146\n",
      "tensor(0.0220, grad_fn=<MseLossBackward0>)\n",
      "159 0.021978698670864105\n",
      "tensor(0.0217, grad_fn=<MseLossBackward0>)\n",
      "160 0.02173325978219509\n",
      "tensor(0.0215, grad_fn=<MseLossBackward0>)\n",
      "161 0.021490616723895073\n",
      "tensor(0.0213, grad_fn=<MseLossBackward0>)\n",
      "162 0.02125062234699726\n",
      "tensor(0.0210, grad_fn=<MseLossBackward0>)\n",
      "163 0.02101323939859867\n",
      "tensor(0.0208, grad_fn=<MseLossBackward0>)\n",
      "164 0.02077861875295639\n",
      "tensor(0.0205, grad_fn=<MseLossBackward0>)\n",
      "165 0.020546644926071167\n",
      "tensor(0.0203, grad_fn=<MseLossBackward0>)\n",
      "166 0.020317155867815018\n",
      "tensor(0.0201, grad_fn=<MseLossBackward0>)\n",
      "167 0.02009030431509018\n",
      "tensor(0.0199, grad_fn=<MseLossBackward0>)\n",
      "168 0.019865931943058968\n",
      "tensor(0.0196, grad_fn=<MseLossBackward0>)\n",
      "169 0.019644103944301605\n",
      "tensor(0.0194, grad_fn=<MseLossBackward0>)\n",
      "170 0.019424745813012123\n",
      "tensor(0.0192, grad_fn=<MseLossBackward0>)\n",
      "171 0.01920783333480358\n",
      "tensor(0.0190, grad_fn=<MseLossBackward0>)\n",
      "172 0.01899329572916031\n",
      "tensor(0.0188, grad_fn=<MseLossBackward0>)\n",
      "173 0.018781226128339767\n",
      "tensor(0.0186, grad_fn=<MseLossBackward0>)\n",
      "174 0.018571585416793823\n",
      "tensor(0.0184, grad_fn=<MseLossBackward0>)\n",
      "175 0.01836414448916912\n",
      "tensor(0.0182, grad_fn=<MseLossBackward0>)\n",
      "176 0.018159087747335434\n",
      "tensor(0.0180, grad_fn=<MseLossBackward0>)\n",
      "177 0.017956295982003212\n",
      "tensor(0.0178, grad_fn=<MseLossBackward0>)\n",
      "178 0.017755793407559395\n",
      "tensor(0.0176, grad_fn=<MseLossBackward0>)\n",
      "179 0.017557496204972267\n",
      "tensor(0.0174, grad_fn=<MseLossBackward0>)\n",
      "180 0.017361395061016083\n",
      "tensor(0.0172, grad_fn=<MseLossBackward0>)\n",
      "181 0.01716759242117405\n",
      "tensor(0.0170, grad_fn=<MseLossBackward0>)\n",
      "182 0.016975920647382736\n",
      "tensor(0.0168, grad_fn=<MseLossBackward0>)\n",
      "183 0.01678631268441677\n",
      "tensor(0.0166, grad_fn=<MseLossBackward0>)\n",
      "184 0.016598908230662346\n",
      "tensor(0.0164, grad_fn=<MseLossBackward0>)\n",
      "185 0.016413507983088493\n",
      "tensor(0.0162, grad_fn=<MseLossBackward0>)\n",
      "186 0.016230272129178047\n",
      "tensor(0.0160, grad_fn=<MseLossBackward0>)\n",
      "187 0.016048960387706757\n",
      "tensor(0.0159, grad_fn=<MseLossBackward0>)\n",
      "188 0.01586977019906044\n",
      "tensor(0.0157, grad_fn=<MseLossBackward0>)\n",
      "189 0.01569254882633686\n",
      "tensor(0.0155, grad_fn=<MseLossBackward0>)\n",
      "190 0.01551732700318098\n",
      "tensor(0.0153, grad_fn=<MseLossBackward0>)\n",
      "191 0.01534405630081892\n",
      "tensor(0.0152, grad_fn=<MseLossBackward0>)\n",
      "192 0.015172687359154224\n",
      "tensor(0.0150, grad_fn=<MseLossBackward0>)\n",
      "193 0.015003209933638573\n",
      "tensor(0.0148, grad_fn=<MseLossBackward0>)\n",
      "194 0.014835711568593979\n",
      "tensor(0.0147, grad_fn=<MseLossBackward0>)\n",
      "195 0.014670046977698803\n",
      "tensor(0.0145, grad_fn=<MseLossBackward0>)\n",
      "196 0.014506202191114426\n",
      "tensor(0.0143, grad_fn=<MseLossBackward0>)\n",
      "197 0.014344209805130959\n",
      "tensor(0.0142, grad_fn=<MseLossBackward0>)\n",
      "198 0.01418403908610344\n",
      "tensor(0.0140, grad_fn=<MseLossBackward0>)\n",
      "199 0.014025649055838585\n",
      "tensor(0.0139, grad_fn=<MseLossBackward0>)\n",
      "200 0.013869008980691433\n",
      "tensor(0.0137, grad_fn=<MseLossBackward0>)\n",
      "201 0.013714183121919632\n",
      "tensor(0.0136, grad_fn=<MseLossBackward0>)\n",
      "202 0.013561039231717587\n",
      "tensor(0.0134, grad_fn=<MseLossBackward0>)\n",
      "203 0.013409626670181751\n",
      "tensor(0.0133, grad_fn=<MseLossBackward0>)\n",
      "204 0.013259891420602798\n",
      "tensor(0.0131, grad_fn=<MseLossBackward0>)\n",
      "205 0.013111800886690617\n",
      "tensor(0.0130, grad_fn=<MseLossBackward0>)\n",
      "206 0.012965401634573936\n",
      "tensor(0.0128, grad_fn=<MseLossBackward0>)\n",
      "207 0.012820587493479252\n",
      "tensor(0.0127, grad_fn=<MseLossBackward0>)\n",
      "208 0.012677466496825218\n",
      "tensor(0.0125, grad_fn=<MseLossBackward0>)\n",
      "209 0.012535879388451576\n",
      "tensor(0.0124, grad_fn=<MseLossBackward0>)\n",
      "210 0.012395858764648438\n",
      "tensor(0.0123, grad_fn=<MseLossBackward0>)\n",
      "211 0.012257411144673824\n",
      "tensor(0.0121, grad_fn=<MseLossBackward0>)\n",
      "212 0.012120554223656654\n",
      "tensor(0.0120, grad_fn=<MseLossBackward0>)\n",
      "213 0.011985228396952152\n",
      "tensor(0.0119, grad_fn=<MseLossBackward0>)\n",
      "214 0.01185139175504446\n",
      "tensor(0.0117, grad_fn=<MseLossBackward0>)\n",
      "215 0.011719065718352795\n",
      "tensor(0.0116, grad_fn=<MseLossBackward0>)\n",
      "216 0.01158814411610365\n",
      "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "217 0.01145873125642538\n",
      "tensor(0.0113, grad_fn=<MseLossBackward0>)\n",
      "218 0.011330784298479557\n",
      "tensor(0.0112, grad_fn=<MseLossBackward0>)\n",
      "219 0.011204303242266178\n",
      "tensor(0.0111, grad_fn=<MseLossBackward0>)\n",
      "220 0.011079180054366589\n",
      "tensor(0.0110, grad_fn=<MseLossBackward0>)\n",
      "221 0.010955468751490116\n",
      "tensor(0.0108, grad_fn=<MseLossBackward0>)\n",
      "222 0.010833118110895157\n",
      "tensor(0.0107, grad_fn=<MseLossBackward0>)\n",
      "223 0.01071209367364645\n",
      "tensor(0.0106, grad_fn=<MseLossBackward0>)\n",
      "224 0.010592526756227016\n",
      "tensor(0.0105, grad_fn=<MseLossBackward0>)\n",
      "225 0.01047423668205738\n",
      "tensor(0.0104, grad_fn=<MseLossBackward0>)\n",
      "226 0.010357261635363102\n",
      "tensor(0.0102, grad_fn=<MseLossBackward0>)\n",
      "227 0.01024160161614418\n",
      "tensor(0.0101, grad_fn=<MseLossBackward0>)\n",
      "228 0.010127261281013489\n",
      "tensor(0.0100, grad_fn=<MseLossBackward0>)\n",
      "229 0.010014130733907223\n",
      "tensor(0.0099, grad_fn=<MseLossBackward0>)\n",
      "230 0.009902344085276127\n",
      "tensor(0.0098, grad_fn=<MseLossBackward0>)\n",
      "231 0.009791708551347256\n",
      "tensor(0.0097, grad_fn=<MseLossBackward0>)\n",
      "232 0.009682441130280495\n",
      "tensor(0.0096, grad_fn=<MseLossBackward0>)\n",
      "233 0.009574275463819504\n",
      "tensor(0.0095, grad_fn=<MseLossBackward0>)\n",
      "234 0.00946736428886652\n",
      "tensor(0.0094, grad_fn=<MseLossBackward0>)\n",
      "235 0.009361686185002327\n",
      "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "236 0.009257094003260136\n",
      "tensor(0.0092, grad_fn=<MseLossBackward0>)\n",
      "237 0.009153701364994049\n",
      "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "238 0.009051536209881306\n",
      "tensor(0.0090, grad_fn=<MseLossBackward0>)\n",
      "239 0.00895041972398758\n",
      "tensor(0.0089, grad_fn=<MseLossBackward0>)\n",
      "240 0.008850526995956898\n",
      "tensor(0.0088, grad_fn=<MseLossBackward0>)\n",
      "241 0.008751630783081055\n",
      "tensor(0.0087, grad_fn=<MseLossBackward0>)\n",
      "242 0.008653923869132996\n",
      "tensor(0.0086, grad_fn=<MseLossBackward0>)\n",
      "243 0.008557301014661789\n",
      "tensor(0.0085, grad_fn=<MseLossBackward0>)\n",
      "244 0.00846173707395792\n",
      "tensor(0.0084, grad_fn=<MseLossBackward0>)\n",
      "245 0.008367244154214859\n",
      "tensor(0.0083, grad_fn=<MseLossBackward0>)\n",
      "246 0.008273770101368427\n",
      "tensor(0.0082, grad_fn=<MseLossBackward0>)\n",
      "247 0.008181425742805004\n",
      "tensor(0.0081, grad_fn=<MseLossBackward0>)\n",
      "248 0.008090101182460785\n",
      "tensor(0.0080, grad_fn=<MseLossBackward0>)\n",
      "249 0.007999696768820286\n",
      "tensor(0.0079, grad_fn=<MseLossBackward0>)\n",
      "250 0.0079104108735919\n",
      "tensor(0.0078, grad_fn=<MseLossBackward0>)\n",
      "251 0.007822055369615555\n",
      "tensor(0.0077, grad_fn=<MseLossBackward0>)\n",
      "252 0.007734714075922966\n",
      "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "253 0.00764834089204669\n",
      "tensor(0.0076, grad_fn=<MseLossBackward0>)\n",
      "254 0.007562922313809395\n",
      "tensor(0.0075, grad_fn=<MseLossBackward0>)\n",
      "255 0.007478489074856043\n",
      "tensor(0.0074, grad_fn=<MseLossBackward0>)\n",
      "256 0.007394947111606598\n",
      "tensor(0.0073, grad_fn=<MseLossBackward0>)\n",
      "257 0.00731242448091507\n",
      "tensor(0.0072, grad_fn=<MseLossBackward0>)\n",
      "258 0.007230712566524744\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "259 0.007149992510676384\n",
      "tensor(0.0071, grad_fn=<MseLossBackward0>)\n",
      "260 0.007070166990160942\n",
      "tensor(0.0070, grad_fn=<MseLossBackward0>)\n",
      "261 0.006991174537688494\n",
      "tensor(0.0069, grad_fn=<MseLossBackward0>)\n",
      "262 0.006913095712661743\n",
      "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "263 0.0068359375\n",
      "tensor(0.0068, grad_fn=<MseLossBackward0>)\n",
      "264 0.0067595914006233215\n",
      "tensor(0.0067, grad_fn=<MseLossBackward0>)\n",
      "265 0.00668413657695055\n",
      "tensor(0.0066, grad_fn=<MseLossBackward0>)\n",
      "266 0.006609437055885792\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "267 0.006535662803798914\n",
      "tensor(0.0065, grad_fn=<MseLossBackward0>)\n",
      "268 0.006462689023464918\n",
      "tensor(0.0064, grad_fn=<MseLossBackward0>)\n",
      "269 0.006390504073351622\n",
      "tensor(0.0063, grad_fn=<MseLossBackward0>)\n",
      "270 0.0063191247172653675\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "271 0.006248600780963898\n",
      "tensor(0.0062, grad_fn=<MseLossBackward0>)\n",
      "272 0.006178788375109434\n",
      "tensor(0.0061, grad_fn=<MseLossBackward0>)\n",
      "273 0.006109825801104307\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "274 0.006041580345481634\n",
      "tensor(0.0060, grad_fn=<MseLossBackward0>)\n",
      "275 0.005974145140498877\n",
      "tensor(0.0059, grad_fn=<MseLossBackward0>)\n",
      "276 0.005907407030463219\n",
      "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "277 0.005841447040438652\n",
      "tensor(0.0058, grad_fn=<MseLossBackward0>)\n",
      "278 0.005776196252554655\n",
      "tensor(0.0057, grad_fn=<MseLossBackward0>)\n",
      "279 0.005711709149181843\n",
      "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "280 0.005647913087159395\n",
      "tensor(0.0056, grad_fn=<MseLossBackward0>)\n",
      "281 0.0055848462507128716\n",
      "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "282 0.005522478837519884\n",
      "tensor(0.0055, grad_fn=<MseLossBackward0>)\n",
      "283 0.005460788030177355\n",
      "tensor(0.0054, grad_fn=<MseLossBackward0>)\n",
      "284 0.005399851128458977\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "285 0.005339558701962233\n",
      "tensor(0.0053, grad_fn=<MseLossBackward0>)\n",
      "286 0.005279928911477327\n",
      "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "287 0.0052209701389074326\n",
      "tensor(0.0052, grad_fn=<MseLossBackward0>)\n",
      "288 0.005162664223462343\n",
      "tensor(0.0051, grad_fn=<MseLossBackward0>)\n",
      "289 0.005104988347738981\n",
      "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "290 0.00504800071939826\n",
      "tensor(0.0050, grad_fn=<MseLossBackward0>)\n",
      "291 0.004991630557924509\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "292 0.004935892764478922\n",
      "tensor(0.0049, grad_fn=<MseLossBackward0>)\n",
      "293 0.004880789201706648\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "294 0.004826236050575972\n",
      "tensor(0.0048, grad_fn=<MseLossBackward0>)\n",
      "295 0.004772361367940903\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "296 0.004719075281172991\n",
      "tensor(0.0047, grad_fn=<MseLossBackward0>)\n",
      "297 0.004666349850594997\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "298 0.00461428752169013\n",
      "tensor(0.0046, grad_fn=<MseLossBackward0>)\n",
      "299 0.00456276535987854\n",
      "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "300 0.004511806182563305\n",
      "tensor(0.0045, grad_fn=<MseLossBackward0>)\n",
      "301 0.004461390897631645\n",
      "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "302 0.004411584697663784\n",
      "tensor(0.0044, grad_fn=<MseLossBackward0>)\n",
      "303 0.004362327046692371\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "304 0.004313639365136623\n",
      "tensor(0.0043, grad_fn=<MseLossBackward0>)\n",
      "305 0.004265430383384228\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "306 0.0042178272269666195\n",
      "tensor(0.0042, grad_fn=<MseLossBackward0>)\n",
      "307 0.004170751664787531\n",
      "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "308 0.004124150145798922\n",
      "tensor(0.0041, grad_fn=<MseLossBackward0>)\n",
      "309 0.004078098107129335\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "310 0.004032569006085396\n",
      "tensor(0.0040, grad_fn=<MseLossBackward0>)\n",
      "311 0.003987519070506096\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "312 0.003942999057471752\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "313 0.0038989605382084846\n",
      "tensor(0.0039, grad_fn=<MseLossBackward0>)\n",
      "314 0.003855448216199875\n",
      "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "315 0.003812395967543125\n",
      "tensor(0.0038, grad_fn=<MseLossBackward0>)\n",
      "316 0.0037698119413107634\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "317 0.003727701958268881\n",
      "tensor(0.0037, grad_fn=<MseLossBackward0>)\n",
      "318 0.0036860525142401457\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "319 0.003644918790087104\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "320 0.0036042137071490288\n",
      "tensor(0.0036, grad_fn=<MseLossBackward0>)\n",
      "321 0.0035639724228531122\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "322 0.0035241746809333563\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "323 0.003484832588583231\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "324 0.0034458900336176157\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "325 0.003407403826713562\n",
      "tensor(0.0034, grad_fn=<MseLossBackward0>)\n",
      "326 0.0033693646546453238\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "327 0.0033317545894533396\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "328 0.003294561756774783\n",
      "tensor(0.0033, grad_fn=<MseLossBackward0>)\n",
      "329 0.003257765667513013\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "330 0.0032213658560067415\n",
      "tensor(0.0032, grad_fn=<MseLossBackward0>)\n",
      "331 0.003185403300449252\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "332 0.0031498337630182505\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "333 0.00311464280821383\n",
      "tensor(0.0031, grad_fn=<MseLossBackward0>)\n",
      "334 0.0030798560474067926\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "335 0.0030454685911536217\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "336 0.003011464374139905\n",
      "tensor(0.0030, grad_fn=<MseLossBackward0>)\n",
      "337 0.0029778664465993643\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "338 0.0029445781838148832\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "339 0.0029117008671164513\n",
      "tensor(0.0029, grad_fn=<MseLossBackward0>)\n",
      "340 0.0028792209923267365\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "341 0.0028470305260270834\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "342 0.0028152407612651587\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "343 0.0027838232927024364\n",
      "tensor(0.0028, grad_fn=<MseLossBackward0>)\n",
      "344 0.002752715954557061\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "345 0.002721978584304452\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "346 0.002691588830202818\n",
      "tensor(0.0027, grad_fn=<MseLossBackward0>)\n",
      "347 0.002661535982042551\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "348 0.0026318158488720655\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "349 0.0026024160906672478\n",
      "tensor(0.0026, grad_fn=<MseLossBackward0>)\n",
      "350 0.0025733711663633585\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "351 0.002544654766097665\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "352 0.0025162368547171354\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "353 0.0024881099816411734\n",
      "tensor(0.0025, grad_fn=<MseLossBackward0>)\n",
      "354 0.0024603342171758413\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "355 0.0024328401777893305\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "356 0.002405689097940922\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "357 0.002378848847001791\n",
      "tensor(0.0024, grad_fn=<MseLossBackward0>)\n",
      "358 0.0023522344417870045\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "359 0.0023260097950696945\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "360 0.002300030319020152\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "361 0.002274336526170373\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "362 0.0022489384282380342\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "363 0.002223813673481345\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "364 0.0021989806555211544\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "365 0.0021744391415268183\n",
      "tensor(0.0022, grad_fn=<MseLossBackward0>)\n",
      "366 0.0021501495502889156\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "367 0.002126127015799284\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "368 0.0021024078596383333\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "369 0.002078916411846876\n",
      "tensor(0.0021, grad_fn=<MseLossBackward0>)\n",
      "370 0.002055720891803503\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "371 0.0020327577367424965\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "372 0.0020100397523492575\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "373 0.001987629570066929\n",
      "tensor(0.0020, grad_fn=<MseLossBackward0>)\n",
      "374 0.001965401228517294\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "375 0.001943468232639134\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "376 0.0019217609660699964\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "377 0.001900313887745142\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "378 0.0018790822941809893\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "379 0.0018580964533612132\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "380 0.001837344840168953\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "381 0.00181682372931391\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "382 0.0017965391743928194\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "383 0.0017764803487807512\n",
      "tensor(0.0018, grad_fn=<MseLossBackward0>)\n",
      "384 0.001756640151143074\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "385 0.0017370465211570263\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "386 0.0017176202964037657\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "387 0.0016984615940600634\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "388 0.0016794707626104355\n",
      "tensor(0.0017, grad_fn=<MseLossBackward0>)\n",
      "389 0.0016607266152277589\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "390 0.0016421846812590957\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "391 0.0016238552052527666\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "392 0.0016057259635999799\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "393 0.001587791834026575\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "394 0.0015700508374720812\n",
      "tensor(0.0016, grad_fn=<MseLossBackward0>)\n",
      "395 0.0015525069320574403\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "396 0.0015351914335042238\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "397 0.0015180460177361965\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "398 0.0015010867500677705\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "399 0.0014843400567770004\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "400 0.0014677430735900998\n",
      "tensor(0.0015, grad_fn=<MseLossBackward0>)\n",
      "401 0.001451382297091186\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "402 0.0014351620338857174\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "403 0.001419133972376585\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "404 0.0014033018378540874\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "405 0.001387599273584783\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "406 0.0013721291907131672\n",
      "tensor(0.0014, grad_fn=<MseLossBackward0>)\n",
      "407 0.0013567827409133315\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "408 0.0013416586443781853\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "409 0.0013266594614833593\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "410 0.0013118511997163296\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "411 0.0012972044060006738\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "412 0.0012827268801629543\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "413 0.0012683782260864973\n",
      "tensor(0.0013, grad_fn=<MseLossBackward0>)\n",
      "414 0.0012542291078716516\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "415 0.0012402150314301252\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "416 0.0012263747630640864\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "417 0.0012126750079914927\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "418 0.001199136022478342\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "419 0.0011857327772304416\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "420 0.0011725068325176835\n",
      "tensor(0.0012, grad_fn=<MseLossBackward0>)\n",
      "421 0.001159418374300003\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "422 0.0011464681010693312\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "423 0.0011336528696119785\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "424 0.0011209999211132526\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "425 0.001108483993448317\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "426 0.0010961050866171718\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "427 0.0010838729795068502\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "428 0.0010717823170125484\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "429 0.0010597854852676392\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "430 0.0010479559423401952\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "431 0.0010362574830651283\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "432 0.001024689874611795\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "433 0.0010132588213309646\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "434 0.0010019192704930902\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "435 0.0009907360654324293\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "436 0.0009796810336411\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "437 0.0009687338606454432\n",
      "tensor(0.0010, grad_fn=<MseLossBackward0>)\n",
      "438 0.0009579230681993067\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "439 0.0009472340461798012\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "440 0.0009366447338834405\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "441 0.0009261881350539625\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "442 0.0009158426546491683\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "443 0.0009056142880581319\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "444 0.0008954911027103662\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "445 0.0008855027845129371\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "446 0.0008756131283007562\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "447 0.0008658423903398216\n",
      "tensor(0.0009, grad_fn=<MseLossBackward0>)\n",
      "448 0.0008561696740798652\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "449 0.0008466044091619551\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "450 0.0008371577132493258\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "451 0.0008278187015093863\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "452 0.00081857037730515\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "453 0.0008094296208582819\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "454 0.0008003853727132082\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "455 0.0007914461311884224\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "456 0.0007826050277799368\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "457 0.000773856823798269\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "458 0.0007652229978702962\n",
      "tensor(0.0008, grad_fn=<MseLossBackward0>)\n",
      "459 0.0007566794520244002\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "460 0.0007482364890165627\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "461 0.0007398732122965157\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "462 0.000731622742023319\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "463 0.0007234434015117586\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "464 0.0007153695914894342\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "465 0.0007073769229464233\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "466 0.0006994830328039825\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "467 0.0006916677230037749\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "468 0.000683940073940903\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "469 0.0006763021810911596\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "470 0.0006687488057650626\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "471 0.0006612835568375885\n",
      "tensor(0.0007, grad_fn=<MseLossBackward0>)\n",
      "472 0.0006539037567563355\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "473 0.0006466101622208953\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "474 0.0006393787916749716\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "475 0.0006322407280094922\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "476 0.0006251774029806256\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "477 0.0006181923090480268\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "478 0.0006112907431088388\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "479 0.0006044659530743957\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "480 0.0005977184628136456\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "481 0.000591048039495945\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "482 0.0005844492116011679\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "483 0.0005779110360890627\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "484 0.0005714578437618911\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "485 0.0005650920211337507\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "486 0.0005587677005678415\n",
      "tensor(0.0006, grad_fn=<MseLossBackward0>)\n",
      "487 0.0005525298765860498\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "488 0.0005463614361360669\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "489 0.0005402542883530259\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "490 0.0005342234508134425\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "491 0.0005282560014165938\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "492 0.0005223509506322443\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "493 0.0005165267502889037\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "494 0.0005107664619572461\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "495 0.000505056872498244\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "496 0.0004994167829863727\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "497 0.0004938433412462473\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "498 0.000488321646116674\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "499 0.0004828688979614526\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "500 0.00047747668577358127\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "501 0.00047214794903993607\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "502 0.00046687584836035967\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "503 0.0004616662045009434\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "504 0.00045651046093553305\n",
      "tensor(0.0005, grad_fn=<MseLossBackward0>)\n",
      "505 0.0004514111205935478\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "506 0.00044636905658990145\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "507 0.0004413816786836833\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "508 0.0004364554479252547\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "509 0.000431576743721962\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "510 0.00042676401790231466\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "511 0.000421990203903988\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "512 0.00041728050564415753\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "513 0.00041261466685682535\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "514 0.0004080154758412391\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "515 0.00040346282185055315\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "516 0.0003989516117144376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "517 0.00039450309122912586\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "518 0.00039009356987662613\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "519 0.0003857315459754318\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "520 0.0003814292431343347\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "521 0.0003771660849452019\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "522 0.000372959126252681\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "523 0.00036879556137137115\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "524 0.00036467835889197886\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "525 0.0003606077516451478\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "526 0.00035657521220855415\n",
      "tensor(0.0004, grad_fn=<MseLossBackward0>)\n",
      "527 0.00035258749267086387\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "528 0.00034866423811763525\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "529 0.00034476155997253954\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "530 0.00034091711859218776\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "531 0.0003371106868144125\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "532 0.00033334200270473957\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "533 0.00032962477416731417\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "534 0.0003259372606407851\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "535 0.0003222926752641797\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "536 0.0003187036490999162\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "537 0.0003151434357278049\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "538 0.0003116294101346284\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "539 0.0003081474278587848\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "540 0.0003047026984859258\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "541 0.0003013013338204473\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "542 0.0002979378623422235\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "543 0.000294607161777094\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "544 0.0002913226489908993\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "545 0.0002880655520129949\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "546 0.0002848522271960974\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "547 0.00028167373966425657\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "548 0.000278522347798571\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "549 0.0002754181332420558\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "550 0.00027234494336880744\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "551 0.00026929809246212244\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "552 0.00026628654450178146\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "553 0.0002633173717185855\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "554 0.0002603756438475102\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "555 0.00025746459141373634\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "556 0.00025459047174081206\n",
      "tensor(0.0003, grad_fn=<MseLossBackward0>)\n",
      "557 0.00025174624170176685\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "558 0.0002489415055606514\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "559 0.00024615737493149936\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "560 0.0002434028428979218\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "561 0.00024069032224360853\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "562 0.0002380036748945713\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "563 0.00023534736828878522\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "564 0.0002327161782886833\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "565 0.00023011822486296296\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "566 0.00022754562087357044\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "567 0.000225007432163693\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "568 0.00022249631001614034\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "569 0.00022000631724949926\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "570 0.00021755181660410017\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "571 0.00021512429520953447\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "572 0.00021271765581332147\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "573 0.0002103460137732327\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "574 0.0002080003177979961\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "575 0.00020567224419210106\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "576 0.00020337905152700841\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "577 0.0002011034666793421\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "578 0.00019886436348315328\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "579 0.00019664567662402987\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "580 0.00019445078214630485\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "581 0.00019227313168812543\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "582 0.00019012489065062255\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "583 0.0001880044728750363\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "584 0.00018590340914670378\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "585 0.0001838270982261747\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "586 0.00018177578749600798\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "587 0.0001797475415514782\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "588 0.00017773552099242806\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "589 0.00017575918172951788\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "590 0.0001737874117679894\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "591 0.00017185175966005772\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "592 0.0001699309650575742\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "593 0.00016803220205474645\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "594 0.00016616225184407085\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "595 0.0001642999122850597\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "596 0.00016246651648543775\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "597 0.00016065244562923908\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "598 0.00015885444008745253\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "599 0.00015708828868810087\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "600 0.00015532552788499743\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "601 0.0001535971532575786\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "602 0.00015188072575256228\n",
      "tensor(0.0002, grad_fn=<MseLossBackward0>)\n",
      "603 0.0001501916121924296\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "604 0.00014850559819024056\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "605 0.00014685123460367322\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "606 0.00014520516560878605\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "607 0.00014359054330270737\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "608 0.0001419797190465033\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "609 0.00014039866800885648\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "610 0.00013882794883102179\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "611 0.00013728468911722302\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "612 0.00013574586773756891\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "613 0.00013423153723124415\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "614 0.00013273494550958276\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "615 0.00013124912220519036\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "616 0.0001297874259762466\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "617 0.00012833272921852767\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "618 0.00012689971481449902\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "619 0.00012548168888315558\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "620 0.00012408602924551815\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "621 0.00012269822764210403\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "622 0.0001213311479659751\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "623 0.00011997114052064717\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "624 0.00011863427062053233\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "625 0.00011730846017599106\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "626 0.00011599843128351495\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "627 0.00011470550089143217\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "628 0.0001134245831053704\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "629 0.00011215763515792787\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "630 0.00011090277985204011\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "631 0.000109664470073767\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "632 0.00010844602365978062\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "633 0.0001072304803528823\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "634 0.00010603648115647957\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "635 0.00010484537779120728\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "636 0.00010367422510171309\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "637 0.00010251624189550057\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "638 0.0001013751098071225\n",
      "tensor(0.0001, grad_fn=<MseLossBackward0>)\n",
      "639 0.00010024078073911369\n",
      "tensor(9.9124e-05, grad_fn=<MseLossBackward0>)\n",
      "640 9.91235428955406e-05\n",
      "tensor(9.8019e-05, grad_fn=<MseLossBackward0>)\n",
      "641 9.801860869629309e-05\n",
      "tensor(9.6921e-05, grad_fn=<MseLossBackward0>)\n",
      "642 9.692062303656712e-05\n",
      "tensor(9.5840e-05, grad_fn=<MseLossBackward0>)\n",
      "643 9.58397940848954e-05\n",
      "tensor(9.4772e-05, grad_fn=<MseLossBackward0>)\n",
      "644 9.477217099629343e-05\n",
      "tensor(9.3715e-05, grad_fn=<MseLossBackward0>)\n",
      "645 9.371453779749572e-05\n",
      "tensor(9.2667e-05, grad_fn=<MseLossBackward0>)\n",
      "646 9.266722918255255e-05\n",
      "tensor(9.1635e-05, grad_fn=<MseLossBackward0>)\n",
      "647 9.16349163162522e-05\n",
      "tensor(9.0608e-05, grad_fn=<MseLossBackward0>)\n",
      "648 9.060808952199295e-05\n",
      "tensor(8.9594e-05, grad_fn=<MseLossBackward0>)\n",
      "649 8.959420665632933e-05\n",
      "tensor(8.8592e-05, grad_fn=<MseLossBackward0>)\n",
      "650 8.859238005243242e-05\n",
      "tensor(8.7605e-05, grad_fn=<MseLossBackward0>)\n",
      "651 8.760531636653468e-05\n",
      "tensor(8.6630e-05, grad_fn=<MseLossBackward0>)\n",
      "652 8.663000335218385e-05\n",
      "tensor(8.5661e-05, grad_fn=<MseLossBackward0>)\n",
      "653 8.566123869968578e-05\n",
      "tensor(8.4702e-05, grad_fn=<MseLossBackward0>)\n",
      "654 8.470228931400925e-05\n",
      "tensor(8.3758e-05, grad_fn=<MseLossBackward0>)\n",
      "655 8.37580009829253e-05\n",
      "tensor(8.2822e-05, grad_fn=<MseLossBackward0>)\n",
      "656 8.282227645395324e-05\n",
      "tensor(8.1897e-05, grad_fn=<MseLossBackward0>)\n",
      "657 8.189688378479332e-05\n",
      "tensor(8.0983e-05, grad_fn=<MseLossBackward0>)\n",
      "658 8.098255784716457e-05\n",
      "tensor(8.0078e-05, grad_fn=<MseLossBackward0>)\n",
      "659 8.007844007806852e-05\n",
      "tensor(7.9185e-05, grad_fn=<MseLossBackward0>)\n",
      "660 7.918466144474223e-05\n",
      "tensor(7.8301e-05, grad_fn=<MseLossBackward0>)\n",
      "661 7.830069807823747e-05\n",
      "tensor(7.7428e-05, grad_fn=<MseLossBackward0>)\n",
      "662 7.742812886135653e-05\n",
      "tensor(7.6564e-05, grad_fn=<MseLossBackward0>)\n",
      "663 7.656359230168164e-05\n",
      "tensor(7.5706e-05, grad_fn=<MseLossBackward0>)\n",
      "664 7.570569141535088e-05\n",
      "tensor(7.4861e-05, grad_fn=<MseLossBackward0>)\n",
      "665 7.48612656025216e-05\n",
      "tensor(7.4028e-05, grad_fn=<MseLossBackward0>)\n",
      "666 7.402848859783262e-05\n",
      "tensor(7.3200e-05, grad_fn=<MseLossBackward0>)\n",
      "667 7.32002590666525e-05\n",
      "tensor(7.2382e-05, grad_fn=<MseLossBackward0>)\n",
      "668 7.238183752633631e-05\n",
      "tensor(7.1578e-05, grad_fn=<MseLossBackward0>)\n",
      "669 7.15776695869863e-05\n",
      "tensor(7.0773e-05, grad_fn=<MseLossBackward0>)\n",
      "670 7.07729413989e-05\n",
      "tensor(6.9984e-05, grad_fn=<MseLossBackward0>)\n",
      "671 6.998389289947227e-05\n",
      "tensor(6.9202e-05, grad_fn=<MseLossBackward0>)\n",
      "672 6.920178566360846e-05\n",
      "tensor(6.8430e-05, grad_fn=<MseLossBackward0>)\n",
      "673 6.843027222203091e-05\n",
      "tensor(6.7664e-05, grad_fn=<MseLossBackward0>)\n",
      "674 6.766433943994343e-05\n",
      "tensor(6.6908e-05, grad_fn=<MseLossBackward0>)\n",
      "675 6.690767622785643e-05\n",
      "tensor(6.6162e-05, grad_fn=<MseLossBackward0>)\n",
      "676 6.616210157517344e-05\n",
      "tensor(6.5424e-05, grad_fn=<MseLossBackward0>)\n",
      "677 6.542432674905285e-05\n",
      "tensor(6.4690e-05, grad_fn=<MseLossBackward0>)\n",
      "678 6.469012441812083e-05\n",
      "tensor(6.3972e-05, grad_fn=<MseLossBackward0>)\n",
      "679 6.397218385245651e-05\n",
      "tensor(6.3257e-05, grad_fn=<MseLossBackward0>)\n",
      "680 6.325745926005766e-05\n",
      "tensor(6.2549e-05, grad_fn=<MseLossBackward0>)\n",
      "681 6.254910840652883e-05\n",
      "tensor(6.1853e-05, grad_fn=<MseLossBackward0>)\n",
      "682 6.18532722000964e-05\n",
      "tensor(6.1158e-05, grad_fn=<MseLossBackward0>)\n",
      "683 6.115833093645051e-05\n",
      "tensor(6.0478e-05, grad_fn=<MseLossBackward0>)\n",
      "684 6.04776905674953e-05\n",
      "tensor(5.9803e-05, grad_fn=<MseLossBackward0>)\n",
      "685 5.98031074332539e-05\n",
      "tensor(5.9133e-05, grad_fn=<MseLossBackward0>)\n",
      "686 5.9132740716449916e-05\n",
      "tensor(5.8473e-05, grad_fn=<MseLossBackward0>)\n",
      "687 5.8473142416914925e-05\n",
      "tensor(5.7820e-05, grad_fn=<MseLossBackward0>)\n",
      "688 5.7820157962851226e-05\n",
      "tensor(5.7175e-05, grad_fn=<MseLossBackward0>)\n",
      "689 5.717468593502417e-05\n",
      "tensor(5.6539e-05, grad_fn=<MseLossBackward0>)\n",
      "690 5.653854532283731e-05\n",
      "tensor(5.5905e-05, grad_fn=<MseLossBackward0>)\n",
      "691 5.5905456974869594e-05\n",
      "tensor(5.5280e-05, grad_fn=<MseLossBackward0>)\n",
      "692 5.528027031687088e-05\n",
      "tensor(5.4664e-05, grad_fn=<MseLossBackward0>)\n",
      "693 5.466367656481452e-05\n",
      "tensor(5.4052e-05, grad_fn=<MseLossBackward0>)\n",
      "694 5.4051506594987586e-05\n",
      "tensor(5.3451e-05, grad_fn=<MseLossBackward0>)\n",
      "695 5.345074168872088e-05\n",
      "tensor(5.2854e-05, grad_fn=<MseLossBackward0>)\n",
      "696 5.285398583509959e-05\n",
      "tensor(5.2262e-05, grad_fn=<MseLossBackward0>)\n",
      "697 5.226191206020303e-05\n",
      "tensor(5.1675e-05, grad_fn=<MseLossBackward0>)\n",
      "698 5.1674633141374215e-05\n",
      "tensor(5.1101e-05, grad_fn=<MseLossBackward0>)\n",
      "699 5.1100632845191285e-05\n",
      "tensor(5.0530e-05, grad_fn=<MseLossBackward0>)\n",
      "700 5.0529884902061895e-05\n",
      "tensor(4.9966e-05, grad_fn=<MseLossBackward0>)\n",
      "701 4.9966292863246053e-05\n",
      "tensor(4.9410e-05, grad_fn=<MseLossBackward0>)\n",
      "702 4.940991129842587e-05\n",
      "tensor(4.8858e-05, grad_fn=<MseLossBackward0>)\n",
      "703 4.885790258413181e-05\n",
      "tensor(4.8312e-05, grad_fn=<MseLossBackward0>)\n",
      "704 4.831244223169051e-05\n",
      "tensor(4.7771e-05, grad_fn=<MseLossBackward0>)\n",
      "705 4.777067442773841e-05\n",
      "tensor(4.7239e-05, grad_fn=<MseLossBackward0>)\n",
      "706 4.723944948636927e-05\n",
      "tensor(4.6710e-05, grad_fn=<MseLossBackward0>)\n",
      "707 4.670965427067131e-05\n",
      "tensor(4.6188e-05, grad_fn=<MseLossBackward0>)\n",
      "708 4.618774255504832e-05\n",
      "tensor(4.5674e-05, grad_fn=<MseLossBackward0>)\n",
      "709 4.567442010738887e-05\n",
      "tensor(4.5163e-05, grad_fn=<MseLossBackward0>)\n",
      "710 4.516270928434096e-05\n",
      "tensor(4.4662e-05, grad_fn=<MseLossBackward0>)\n",
      "711 4.466150494408794e-05\n",
      "tensor(4.4158e-05, grad_fn=<MseLossBackward0>)\n",
      "712 4.415817602421157e-05\n",
      "tensor(4.3667e-05, grad_fn=<MseLossBackward0>)\n",
      "713 4.3667452700901777e-05\n",
      "tensor(4.3178e-05, grad_fn=<MseLossBackward0>)\n",
      "714 4.317842103773728e-05\n",
      "tensor(4.2696e-05, grad_fn=<MseLossBackward0>)\n",
      "715 4.269625060260296e-05\n",
      "tensor(4.2221e-05, grad_fn=<MseLossBackward0>)\n",
      "716 4.2220999603159726e-05\n",
      "tensor(4.1749e-05, grad_fn=<MseLossBackward0>)\n",
      "717 4.1748971852939576e-05\n",
      "tensor(4.1282e-05, grad_fn=<MseLossBackward0>)\n",
      "718 4.128228465560824e-05\n",
      "tensor(4.0824e-05, grad_fn=<MseLossBackward0>)\n",
      "719 4.082405939698219e-05\n",
      "tensor(4.0368e-05, grad_fn=<MseLossBackward0>)\n",
      "720 4.0367965993937105e-05\n",
      "tensor(3.9918e-05, grad_fn=<MseLossBackward0>)\n",
      "721 3.991811172454618e-05\n",
      "tensor(3.9469e-05, grad_fn=<MseLossBackward0>)\n",
      "722 3.946878496208228e-05\n",
      "tensor(3.9030e-05, grad_fn=<MseLossBackward0>)\n",
      "723 3.9029779145494103e-05\n",
      "tensor(3.8594e-05, grad_fn=<MseLossBackward0>)\n",
      "724 3.859380376525223e-05\n",
      "tensor(3.8161e-05, grad_fn=<MseLossBackward0>)\n",
      "725 3.816124444711022e-05\n",
      "tensor(3.7735e-05, grad_fn=<MseLossBackward0>)\n",
      "726 3.773517528316006e-05\n",
      "tensor(3.7315e-05, grad_fn=<MseLossBackward0>)\n",
      "727 3.731475953827612e-05\n",
      "tensor(3.6897e-05, grad_fn=<MseLossBackward0>)\n",
      "728 3.689664663397707e-05\n",
      "tensor(3.6485e-05, grad_fn=<MseLossBackward0>)\n",
      "729 3.6484780139289796e-05\n",
      "tensor(3.6078e-05, grad_fn=<MseLossBackward0>)\n",
      "730 3.607827238738537e-05\n",
      "tensor(3.5675e-05, grad_fn=<MseLossBackward0>)\n",
      "731 3.567522071534768e-05\n",
      "tensor(3.5279e-05, grad_fn=<MseLossBackward0>)\n",
      "732 3.527923399815336e-05\n",
      "tensor(3.4880e-05, grad_fn=<MseLossBackward0>)\n",
      "733 3.488043148536235e-05\n",
      "tensor(3.4494e-05, grad_fn=<MseLossBackward0>)\n",
      "734 3.449447831371799e-05\n",
      "tensor(3.4109e-05, grad_fn=<MseLossBackward0>)\n",
      "735 3.410914359847084e-05\n",
      "tensor(3.3727e-05, grad_fn=<MseLossBackward0>)\n",
      "736 3.372665014467202e-05\n",
      "tensor(3.3351e-05, grad_fn=<MseLossBackward0>)\n",
      "737 3.335058499942534e-05\n",
      "tensor(3.2979e-05, grad_fn=<MseLossBackward0>)\n",
      "738 3.297877628938295e-05\n",
      "tensor(3.2608e-05, grad_fn=<MseLossBackward0>)\n",
      "739 3.260796802351251e-05\n",
      "tensor(3.2247e-05, grad_fn=<MseLossBackward0>)\n",
      "740 3.224682586733252e-05\n",
      "tensor(3.1886e-05, grad_fn=<MseLossBackward0>)\n",
      "741 3.188633490935899e-05\n",
      "tensor(3.1530e-05, grad_fn=<MseLossBackward0>)\n",
      "742 3.152976933051832e-05\n",
      "tensor(3.1179e-05, grad_fn=<MseLossBackward0>)\n",
      "743 3.117855158052407e-05\n",
      "tensor(3.0827e-05, grad_fn=<MseLossBackward0>)\n",
      "744 3.082727562286891e-05\n",
      "tensor(3.0486e-05, grad_fn=<MseLossBackward0>)\n",
      "745 3.048610597033985e-05\n",
      "tensor(3.0145e-05, grad_fn=<MseLossBackward0>)\n",
      "746 3.0144779884722084e-05\n",
      "tensor(2.9806e-05, grad_fn=<MseLossBackward0>)\n",
      "747 2.9805669328197837e-05\n",
      "tensor(2.9475e-05, grad_fn=<MseLossBackward0>)\n",
      "748 2.9474736948031932e-05\n",
      "tensor(2.9146e-05, grad_fn=<MseLossBackward0>)\n",
      "749 2.91457909042947e-05\n",
      "tensor(2.8820e-05, grad_fn=<MseLossBackward0>)\n",
      "750 2.8819964427384548e-05\n",
      "tensor(2.8498e-05, grad_fn=<MseLossBackward0>)\n",
      "751 2.8498470783233643e-05\n",
      "tensor(2.8183e-05, grad_fn=<MseLossBackward0>)\n",
      "752 2.8183008907944895e-05\n",
      "tensor(2.7867e-05, grad_fn=<MseLossBackward0>)\n",
      "753 2.786720870062709e-05\n",
      "tensor(2.7556e-05, grad_fn=<MseLossBackward0>)\n",
      "754 2.7556072382139973e-05\n",
      "tensor(2.7248e-05, grad_fn=<MseLossBackward0>)\n",
      "755 2.7247582693235017e-05\n",
      "tensor(2.6942e-05, grad_fn=<MseLossBackward0>)\n",
      "756 2.6941588657791726e-05\n",
      "tensor(2.6643e-05, grad_fn=<MseLossBackward0>)\n",
      "757 2.6642595912562683e-05\n",
      "tensor(2.6345e-05, grad_fn=<MseLossBackward0>)\n",
      "758 2.63449674093863e-05\n",
      "tensor(2.6050e-05, grad_fn=<MseLossBackward0>)\n",
      "759 2.605037298053503e-05\n",
      "tensor(2.5759e-05, grad_fn=<MseLossBackward0>)\n",
      "760 2.575854887254536e-05\n",
      "tensor(2.5472e-05, grad_fn=<MseLossBackward0>)\n",
      "761 2.5471998014836572e-05\n",
      "tensor(2.5188e-05, grad_fn=<MseLossBackward0>)\n",
      "762 2.5187920982716605e-05\n",
      "tensor(2.4905e-05, grad_fn=<MseLossBackward0>)\n",
      "763 2.4904533347580582e-05\n",
      "tensor(2.4628e-05, grad_fn=<MseLossBackward0>)\n",
      "764 2.462816883053165e-05\n",
      "tensor(2.4353e-05, grad_fn=<MseLossBackward0>)\n",
      "765 2.435284659441095e-05\n",
      "tensor(2.4080e-05, grad_fn=<MseLossBackward0>)\n",
      "766 2.407957254035864e-05\n",
      "tensor(2.3811e-05, grad_fn=<MseLossBackward0>)\n",
      "767 2.3811364371795207e-05\n",
      "tensor(2.3546e-05, grad_fn=<MseLossBackward0>)\n",
      "768 2.3546306692878716e-05\n",
      "tensor(2.3285e-05, grad_fn=<MseLossBackward0>)\n",
      "769 2.3284990675165318e-05\n",
      "tensor(2.3023e-05, grad_fn=<MseLossBackward0>)\n",
      "770 2.302282882737927e-05\n",
      "tensor(2.2765e-05, grad_fn=<MseLossBackward0>)\n",
      "771 2.2765227186027914e-05\n",
      "tensor(2.2512e-05, grad_fn=<MseLossBackward0>)\n",
      "772 2.251247497042641e-05\n",
      "tensor(2.2261e-05, grad_fn=<MseLossBackward0>)\n",
      "773 2.2261303456616588e-05\n",
      "tensor(2.2013e-05, grad_fn=<MseLossBackward0>)\n",
      "774 2.2012691260897554e-05\n",
      "tensor(2.1765e-05, grad_fn=<MseLossBackward0>)\n",
      "775 2.1765174096799456e-05\n",
      "tensor(2.1522e-05, grad_fn=<MseLossBackward0>)\n",
      "776 2.1521673261304386e-05\n",
      "tensor(2.1281e-05, grad_fn=<MseLossBackward0>)\n",
      "777 2.1280722648953088e-05\n",
      "tensor(2.1045e-05, grad_fn=<MseLossBackward0>)\n",
      "778 2.104484156006947e-05\n",
      "tensor(2.0809e-05, grad_fn=<MseLossBackward0>)\n",
      "779 2.0808633053093217e-05\n",
      "tensor(2.0577e-05, grad_fn=<MseLossBackward0>)\n",
      "780 2.0576964743668213e-05\n",
      "tensor(2.0347e-05, grad_fn=<MseLossBackward0>)\n",
      "781 2.0346644305391237e-05\n",
      "tensor(2.0121e-05, grad_fn=<MseLossBackward0>)\n",
      "782 2.0121455236221664e-05\n",
      "tensor(1.9895e-05, grad_fn=<MseLossBackward0>)\n",
      "783 1.989472730201669e-05\n",
      "tensor(1.9673e-05, grad_fn=<MseLossBackward0>)\n",
      "784 1.9673396309372038e-05\n",
      "tensor(1.9454e-05, grad_fn=<MseLossBackward0>)\n",
      "785 1.9453553250059485e-05\n",
      "tensor(1.9235e-05, grad_fn=<MseLossBackward0>)\n",
      "786 1.9235332729294896e-05\n",
      "tensor(1.9020e-05, grad_fn=<MseLossBackward0>)\n",
      "787 1.902024814626202e-05\n",
      "tensor(1.8808e-05, grad_fn=<MseLossBackward0>)\n",
      "788 1.8807790183927864e-05\n",
      "tensor(1.8599e-05, grad_fn=<MseLossBackward0>)\n",
      "789 1.8598553651827388e-05\n",
      "tensor(1.8392e-05, grad_fn=<MseLossBackward0>)\n",
      "790 1.8392061974736862e-05\n",
      "tensor(1.8185e-05, grad_fn=<MseLossBackward0>)\n",
      "791 1.8185390217695385e-05\n",
      "tensor(1.7984e-05, grad_fn=<MseLossBackward0>)\n",
      "792 1.7983789803110994e-05\n",
      "tensor(1.7782e-05, grad_fn=<MseLossBackward0>)\n",
      "793 1.778246405592654e-05\n",
      "tensor(1.7583e-05, grad_fn=<MseLossBackward0>)\n",
      "794 1.7582504369784147e-05\n",
      "tensor(1.7387e-05, grad_fn=<MseLossBackward0>)\n",
      "795 1.73869793798076e-05\n",
      "tensor(1.7193e-05, grad_fn=<MseLossBackward0>)\n",
      "796 1.7193071471410803e-05\n",
      "tensor(1.7002e-05, grad_fn=<MseLossBackward0>)\n",
      "797 1.7001597370835952e-05\n",
      "tensor(1.6812e-05, grad_fn=<MseLossBackward0>)\n",
      "798 1.681180037849117e-05\n",
      "tensor(1.6624e-05, grad_fn=<MseLossBackward0>)\n",
      "799 1.6624155250610784e-05\n",
      "tensor(1.6439e-05, grad_fn=<MseLossBackward0>)\n",
      "800 1.643910763959866e-05\n",
      "tensor(1.6255e-05, grad_fn=<MseLossBackward0>)\n",
      "801 1.625483673706185e-05\n",
      "tensor(1.6072e-05, grad_fn=<MseLossBackward0>)\n",
      "802 1.607224294275511e-05\n",
      "tensor(1.5893e-05, grad_fn=<MseLossBackward0>)\n",
      "803 1.5892877854639664e-05\n",
      "tensor(1.5716e-05, grad_fn=<MseLossBackward0>)\n",
      "804 1.5715733752585948e-05\n",
      "tensor(1.5542e-05, grad_fn=<MseLossBackward0>)\n",
      "805 1.5542387700406834e-05\n",
      "tensor(1.5366e-05, grad_fn=<MseLossBackward0>)\n",
      "806 1.536594209028408e-05\n",
      "tensor(1.5195e-05, grad_fn=<MseLossBackward0>)\n",
      "807 1.5195369087450672e-05\n",
      "tensor(1.5026e-05, grad_fn=<MseLossBackward0>)\n",
      "808 1.5025961147330236e-05\n",
      "tensor(1.4858e-05, grad_fn=<MseLossBackward0>)\n",
      "809 1.4857751921226736e-05\n",
      "tensor(1.4693e-05, grad_fn=<MseLossBackward0>)\n",
      "810 1.4693213415739592e-05\n",
      "tensor(1.4529e-05, grad_fn=<MseLossBackward0>)\n",
      "811 1.4529130567098036e-05\n",
      "tensor(1.4367e-05, grad_fn=<MseLossBackward0>)\n",
      "812 1.4366801224241499e-05\n",
      "tensor(1.4204e-05, grad_fn=<MseLossBackward0>)\n",
      "813 1.420430180587573e-05\n",
      "tensor(1.4047e-05, grad_fn=<MseLossBackward0>)\n",
      "814 1.4046720025362447e-05\n",
      "tensor(1.3891e-05, grad_fn=<MseLossBackward0>)\n",
      "815 1.3890513400838245e-05\n",
      "tensor(1.3735e-05, grad_fn=<MseLossBackward0>)\n",
      "816 1.3734606000070926e-05\n",
      "tensor(1.3582e-05, grad_fn=<MseLossBackward0>)\n",
      "817 1.3582062820205465e-05\n",
      "tensor(1.3430e-05, grad_fn=<MseLossBackward0>)\n",
      "818 1.3429516911855899e-05\n",
      "tensor(1.3279e-05, grad_fn=<MseLossBackward0>)\n",
      "819 1.3279043741931673e-05\n",
      "tensor(1.3131e-05, grad_fn=<MseLossBackward0>)\n",
      "820 1.3131452760717366e-05\n",
      "tensor(1.2985e-05, grad_fn=<MseLossBackward0>)\n",
      "821 1.2984542081539985e-05\n",
      "tensor(1.2840e-05, grad_fn=<MseLossBackward0>)\n",
      "822 1.2839966984756757e-05\n",
      "tensor(1.2694e-05, grad_fn=<MseLossBackward0>)\n",
      "823 1.2694339602603577e-05\n",
      "tensor(1.2554e-05, grad_fn=<MseLossBackward0>)\n",
      "824 1.2553590750030708e-05\n",
      "tensor(1.2413e-05, grad_fn=<MseLossBackward0>)\n",
      "825 1.2412990145094227e-05\n",
      "tensor(1.2277e-05, grad_fn=<MseLossBackward0>)\n",
      "826 1.2276866073079873e-05\n",
      "tensor(1.2138e-05, grad_fn=<MseLossBackward0>)\n",
      "827 1.213778250530595e-05\n",
      "tensor(1.2003e-05, grad_fn=<MseLossBackward0>)\n",
      "828 1.2003277333860751e-05\n",
      "tensor(1.1869e-05, grad_fn=<MseLossBackward0>)\n",
      "829 1.186863210023148e-05\n",
      "tensor(1.1737e-05, grad_fn=<MseLossBackward0>)\n",
      "830 1.1736986380128656e-05\n",
      "tensor(1.1605e-05, grad_fn=<MseLossBackward0>)\n",
      "831 1.1605015060922597e-05\n",
      "tensor(1.1477e-05, grad_fn=<MseLossBackward0>)\n",
      "832 1.147659986600047e-05\n",
      "tensor(1.1348e-05, grad_fn=<MseLossBackward0>)\n",
      "833 1.1347769941494334e-05\n",
      "tensor(1.1221e-05, grad_fn=<MseLossBackward0>)\n",
      "834 1.12210946099367e-05\n",
      "tensor(1.1095e-05, grad_fn=<MseLossBackward0>)\n",
      "835 1.1095426998508628e-05\n",
      "tensor(1.0971e-05, grad_fn=<MseLossBackward0>)\n",
      "836 1.0971372830681503e-05\n",
      "tensor(1.0850e-05, grad_fn=<MseLossBackward0>)\n",
      "837 1.0849747013708111e-05\n",
      "tensor(1.0729e-05, grad_fn=<MseLossBackward0>)\n",
      "838 1.072867053153459e-05\n",
      "tensor(1.0609e-05, grad_fn=<MseLossBackward0>)\n",
      "839 1.060933391272556e-05\n",
      "tensor(1.0489e-05, grad_fn=<MseLossBackward0>)\n",
      "840 1.0488848602108192e-05\n",
      "tensor(1.0372e-05, grad_fn=<MseLossBackward0>)\n",
      "841 1.0372406904934905e-05\n",
      "tensor(1.0257e-05, grad_fn=<MseLossBackward0>)\n",
      "842 1.0257018402626272e-05\n",
      "tensor(1.0144e-05, grad_fn=<MseLossBackward0>)\n",
      "843 1.0143874533241615e-05\n",
      "tensor(1.0029e-05, grad_fn=<MseLossBackward0>)\n",
      "844 1.002869066724088e-05\n",
      "tensor(9.9164e-06, grad_fn=<MseLossBackward0>)\n",
      "845 9.916424460243434e-06\n",
      "tensor(9.8065e-06, grad_fn=<MseLossBackward0>)\n",
      "846 9.806542038859334e-06\n",
      "tensor(9.6973e-06, grad_fn=<MseLossBackward0>)\n",
      "847 9.697338100522757e-06\n",
      "tensor(9.5888e-06, grad_fn=<MseLossBackward0>)\n",
      "848 9.588751709088683e-06\n",
      "tensor(9.4820e-06, grad_fn=<MseLossBackward0>)\n",
      "849 9.481955203227699e-06\n",
      "tensor(9.3752e-06, grad_fn=<MseLossBackward0>)\n",
      "850 9.37524873734219e-06\n",
      "tensor(9.2717e-06, grad_fn=<MseLossBackward0>)\n",
      "851 9.271685485146008e-06\n",
      "tensor(9.1664e-06, grad_fn=<MseLossBackward0>)\n",
      "852 9.166425115836319e-06\n",
      "tensor(9.0650e-06, grad_fn=<MseLossBackward0>)\n",
      "853 9.064951882464811e-06\n",
      "tensor(8.9654e-06, grad_fn=<MseLossBackward0>)\n",
      "854 8.965376764535904e-06\n",
      "tensor(8.8637e-06, grad_fn=<MseLossBackward0>)\n",
      "855 8.86367979546776e-06\n",
      "tensor(8.7639e-06, grad_fn=<MseLossBackward0>)\n",
      "856 8.76392823556671e-06\n",
      "tensor(8.6687e-06, grad_fn=<MseLossBackward0>)\n",
      "857 8.66867685545003e-06\n",
      "tensor(8.5710e-06, grad_fn=<MseLossBackward0>)\n",
      "858 8.571048965677619e-06\n",
      "tensor(8.4745e-06, grad_fn=<MseLossBackward0>)\n",
      "859 8.474470632791054e-06\n",
      "tensor(8.3806e-06, grad_fn=<MseLossBackward0>)\n",
      "860 8.380611689062789e-06\n",
      "tensor(8.2868e-06, grad_fn=<MseLossBackward0>)\n",
      "861 8.286838237836491e-06\n",
      "tensor(8.1944e-06, grad_fn=<MseLossBackward0>)\n",
      "862 8.19442720967345e-06\n",
      "tensor(8.1024e-06, grad_fn=<MseLossBackward0>)\n",
      "863 8.102383617369924e-06\n",
      "tensor(8.0124e-06, grad_fn=<MseLossBackward0>)\n",
      "864 8.012352736841422e-06\n",
      "tensor(7.9219e-06, grad_fn=<MseLossBackward0>)\n",
      "865 7.921858014015015e-06\n",
      "tensor(7.8343e-06, grad_fn=<MseLossBackward0>)\n",
      "866 7.83427276473958e-06\n",
      "tensor(7.7455e-06, grad_fn=<MseLossBackward0>)\n",
      "867 7.745513357804157e-06\n",
      "tensor(7.6613e-06, grad_fn=<MseLossBackward0>)\n",
      "868 7.661339623155072e-06\n",
      "tensor(7.5750e-06, grad_fn=<MseLossBackward0>)\n",
      "869 7.57504312787205e-06\n",
      "tensor(7.4898e-06, grad_fn=<MseLossBackward0>)\n",
      "870 7.489823929063277e-06\n",
      "tensor(7.4064e-06, grad_fn=<MseLossBackward0>)\n",
      "871 7.406447821267648e-06\n",
      "tensor(7.3235e-06, grad_fn=<MseLossBackward0>)\n",
      "872 7.323502813960658e-06\n",
      "tensor(7.2422e-06, grad_fn=<MseLossBackward0>)\n",
      "873 7.2421826189383864e-06\n",
      "tensor(7.1601e-06, grad_fn=<MseLossBackward0>)\n",
      "874 7.160080713219941e-06\n",
      "tensor(7.0815e-06, grad_fn=<MseLossBackward0>)\n",
      "875 7.081473995640408e-06\n",
      "tensor(7.0025e-06, grad_fn=<MseLossBackward0>)\n",
      "876 7.002513939369237e-06\n",
      "tensor(6.9230e-06, grad_fn=<MseLossBackward0>)\n",
      "877 6.923036835360108e-06\n",
      "tensor(6.8474e-06, grad_fn=<MseLossBackward0>)\n",
      "878 6.847360509709688e-06\n",
      "tensor(6.7705e-06, grad_fn=<MseLossBackward0>)\n",
      "879 6.770454547222471e-06\n",
      "tensor(6.6938e-06, grad_fn=<MseLossBackward0>)\n",
      "880 6.693781870126259e-06\n",
      "tensor(6.6207e-06, grad_fn=<MseLossBackward0>)\n",
      "881 6.6207212512381375e-06\n",
      "tensor(6.5467e-06, grad_fn=<MseLossBackward0>)\n",
      "882 6.54673931421712e-06\n",
      "tensor(6.4733e-06, grad_fn=<MseLossBackward0>)\n",
      "883 6.473285793617833e-06\n",
      "tensor(6.4013e-06, grad_fn=<MseLossBackward0>)\n",
      "884 6.4012579059635755e-06\n",
      "tensor(6.3301e-06, grad_fn=<MseLossBackward0>)\n",
      "885 6.3301431509898975e-06\n",
      "tensor(6.2577e-06, grad_fn=<MseLossBackward0>)\n",
      "886 6.257740096771158e-06\n",
      "tensor(6.1885e-06, grad_fn=<MseLossBackward0>)\n",
      "887 6.188474344526185e-06\n",
      "tensor(6.1196e-06, grad_fn=<MseLossBackward0>)\n",
      "888 6.119592399045359e-06\n",
      "tensor(6.0503e-06, grad_fn=<MseLossBackward0>)\n",
      "889 6.050272077118279e-06\n",
      "tensor(5.9834e-06, grad_fn=<MseLossBackward0>)\n",
      "890 5.983407390885986e-06\n",
      "tensor(5.9158e-06, grad_fn=<MseLossBackward0>)\n",
      "891 5.915786005061818e-06\n",
      "tensor(5.8506e-06, grad_fn=<MseLossBackward0>)\n",
      "892 5.850616162206279e-06\n",
      "tensor(5.7855e-06, grad_fn=<MseLossBackward0>)\n",
      "893 5.785522716905689e-06\n",
      "tensor(5.7202e-06, grad_fn=<MseLossBackward0>)\n",
      "894 5.7202096286346205e-06\n",
      "tensor(5.6557e-06, grad_fn=<MseLossBackward0>)\n",
      "895 5.655696440953761e-06\n",
      "tensor(5.5924e-06, grad_fn=<MseLossBackward0>)\n",
      "896 5.592398338194471e-06\n",
      "tensor(5.5299e-06, grad_fn=<MseLossBackward0>)\n",
      "897 5.529919690161478e-06\n",
      "tensor(5.4688e-06, grad_fn=<MseLossBackward0>)\n",
      "898 5.468821655085776e-06\n",
      "tensor(5.4079e-06, grad_fn=<MseLossBackward0>)\n",
      "899 5.407860498962691e-06\n",
      "tensor(5.3476e-06, grad_fn=<MseLossBackward0>)\n",
      "900 5.347636033548042e-06\n",
      "tensor(5.2891e-06, grad_fn=<MseLossBackward0>)\n",
      "901 5.289088221616112e-06\n",
      "tensor(5.2288e-06, grad_fn=<MseLossBackward0>)\n",
      "902 5.228782356425654e-06\n",
      "tensor(5.1704e-06, grad_fn=<MseLossBackward0>)\n",
      "903 5.170362783246674e-06\n",
      "tensor(5.1124e-06, grad_fn=<MseLossBackward0>)\n",
      "904 5.11236203237786e-06\n",
      "tensor(5.0563e-06, grad_fn=<MseLossBackward0>)\n",
      "905 5.0562721298774704e-06\n",
      "tensor(4.9998e-06, grad_fn=<MseLossBackward0>)\n",
      "906 4.999846169084776e-06\n",
      "tensor(4.9430e-06, grad_fn=<MseLossBackward0>)\n",
      "907 4.943014573655091e-06\n",
      "tensor(4.8888e-06, grad_fn=<MseLossBackward0>)\n",
      "908 4.888774128630757e-06\n",
      "tensor(4.8339e-06, grad_fn=<MseLossBackward0>)\n",
      "909 4.833925686398288e-06\n",
      "tensor(4.7801e-06, grad_fn=<MseLossBackward0>)\n",
      "910 4.780106337420875e-06\n",
      "tensor(4.7264e-06, grad_fn=<MseLossBackward0>)\n",
      "911 4.726428869616939e-06\n",
      "tensor(4.6737e-06, grad_fn=<MseLossBackward0>)\n",
      "912 4.673725470638601e-06\n",
      "tensor(4.6220e-06, grad_fn=<MseLossBackward0>)\n",
      "913 4.62202069684281e-06\n",
      "tensor(4.5693e-06, grad_fn=<MseLossBackward0>)\n",
      "914 4.569267275655875e-06\n",
      "tensor(4.5182e-06, grad_fn=<MseLossBackward0>)\n",
      "915 4.51822234026622e-06\n",
      "tensor(4.4682e-06, grad_fn=<MseLossBackward0>)\n",
      "916 4.46823150923592e-06\n",
      "tensor(4.4180e-06, grad_fn=<MseLossBackward0>)\n",
      "917 4.417957370606018e-06\n",
      "tensor(4.3686e-06, grad_fn=<MseLossBackward0>)\n",
      "918 4.368643203633837e-06\n",
      "tensor(4.3196e-06, grad_fn=<MseLossBackward0>)\n",
      "919 4.319579602451995e-06\n",
      "tensor(4.2722e-06, grad_fn=<MseLossBackward0>)\n",
      "920 4.27223812948796e-06\n",
      "tensor(4.2239e-06, grad_fn=<MseLossBackward0>)\n",
      "921 4.223865744279465e-06\n",
      "tensor(4.1764e-06, grad_fn=<MseLossBackward0>)\n",
      "922 4.176438324066112e-06\n",
      "tensor(4.1310e-06, grad_fn=<MseLossBackward0>)\n",
      "923 4.130966772208922e-06\n",
      "tensor(4.0847e-06, grad_fn=<MseLossBackward0>)\n",
      "924 4.084676675120136e-06\n",
      "tensor(4.0392e-06, grad_fn=<MseLossBackward0>)\n",
      "925 4.0391691982222255e-06\n",
      "tensor(3.9934e-06, grad_fn=<MseLossBackward0>)\n",
      "926 3.993442987848539e-06\n",
      "tensor(3.9491e-06, grad_fn=<MseLossBackward0>)\n",
      "927 3.949096480937442e-06\n",
      "tensor(3.9050e-06, grad_fn=<MseLossBackward0>)\n",
      "928 3.905034645868e-06\n",
      "tensor(3.8621e-06, grad_fn=<MseLossBackward0>)\n",
      "929 3.862140147248283e-06\n",
      "tensor(3.8179e-06, grad_fn=<MseLossBackward0>)\n",
      "930 3.817870492639486e-06\n",
      "tensor(3.7756e-06, grad_fn=<MseLossBackward0>)\n",
      "931 3.7755794437543955e-06\n",
      "tensor(3.7340e-06, grad_fn=<MseLossBackward0>)\n",
      "932 3.733988478415995e-06\n",
      "tensor(3.6917e-06, grad_fn=<MseLossBackward0>)\n",
      "933 3.6916799217578955e-06\n",
      "tensor(3.6512e-06, grad_fn=<MseLossBackward0>)\n",
      "934 3.6511848975351313e-06\n",
      "tensor(3.6102e-06, grad_fn=<MseLossBackward0>)\n",
      "935 3.610215344451717e-06\n",
      "tensor(3.5700e-06, grad_fn=<MseLossBackward0>)\n",
      "936 3.5700375065061962e-06\n",
      "tensor(3.5295e-06, grad_fn=<MseLossBackward0>)\n",
      "937 3.5295227007736685e-06\n",
      "tensor(3.4913e-06, grad_fn=<MseLossBackward0>)\n",
      "938 3.4912982300738804e-06\n",
      "tensor(3.4517e-06, grad_fn=<MseLossBackward0>)\n",
      "939 3.4516860978328623e-06\n",
      "tensor(3.4133e-06, grad_fn=<MseLossBackward0>)\n",
      "940 3.4133181543438695e-06\n",
      "tensor(3.3748e-06, grad_fn=<MseLossBackward0>)\n",
      "941 3.374843572601094e-06\n",
      "tensor(3.3367e-06, grad_fn=<MseLossBackward0>)\n",
      "942 3.336708914503106e-06\n",
      "tensor(3.2993e-06, grad_fn=<MseLossBackward0>)\n",
      "943 3.299313448223984e-06\n",
      "tensor(3.2624e-06, grad_fn=<MseLossBackward0>)\n",
      "944 3.2623754577798536e-06\n",
      "tensor(3.2268e-06, grad_fn=<MseLossBackward0>)\n",
      "945 3.226830585845164e-06\n",
      "tensor(3.1903e-06, grad_fn=<MseLossBackward0>)\n",
      "946 3.190312327205902e-06\n",
      "tensor(3.1552e-06, grad_fn=<MseLossBackward0>)\n",
      "947 3.1551564916298958e-06\n",
      "tensor(3.1202e-06, grad_fn=<MseLossBackward0>)\n",
      "948 3.1202469017443946e-06\n",
      "tensor(3.0842e-06, grad_fn=<MseLossBackward0>)\n",
      "949 3.084224545091274e-06\n",
      "tensor(3.0508e-06, grad_fn=<MseLossBackward0>)\n",
      "950 3.0507935662171803e-06\n",
      "tensor(3.0169e-06, grad_fn=<MseLossBackward0>)\n",
      "951 3.0169469482643763e-06\n",
      "tensor(2.9826e-06, grad_fn=<MseLossBackward0>)\n",
      "952 2.982620117109036e-06\n",
      "tensor(2.9499e-06, grad_fn=<MseLossBackward0>)\n",
      "953 2.949866257040412e-06\n",
      "tensor(2.9164e-06, grad_fn=<MseLossBackward0>)\n",
      "954 2.916405946962186e-06\n",
      "tensor(2.8837e-06, grad_fn=<MseLossBackward0>)\n",
      "955 2.8836618639616063e-06\n",
      "tensor(2.8518e-06, grad_fn=<MseLossBackward0>)\n",
      "956 2.851764975275728e-06\n",
      "tensor(2.8197e-06, grad_fn=<MseLossBackward0>)\n",
      "957 2.8197080155223375e-06\n",
      "tensor(2.7883e-06, grad_fn=<MseLossBackward0>)\n",
      "958 2.788267693176749e-06\n",
      "tensor(2.7566e-06, grad_fn=<MseLossBackward0>)\n",
      "959 2.7565793061512522e-06\n",
      "tensor(2.7268e-06, grad_fn=<MseLossBackward0>)\n",
      "960 2.726837465161225e-06\n",
      "tensor(2.6969e-06, grad_fn=<MseLossBackward0>)\n",
      "961 2.696864157769596e-06\n",
      "tensor(2.6663e-06, grad_fn=<MseLossBackward0>)\n",
      "962 2.666308319021482e-06\n",
      "tensor(2.6364e-06, grad_fn=<MseLossBackward0>)\n",
      "963 2.6364459699834697e-06\n",
      "tensor(2.6072e-06, grad_fn=<MseLossBackward0>)\n",
      "964 2.607161150081083e-06\n",
      "tensor(2.5772e-06, grad_fn=<MseLossBackward0>)\n",
      "965 2.577180794105516e-06\n",
      "tensor(2.5488e-06, grad_fn=<MseLossBackward0>)\n",
      "966 2.548813654357218e-06\n",
      "tensor(2.5200e-06, grad_fn=<MseLossBackward0>)\n",
      "967 2.520012685636175e-06\n",
      "tensor(2.4925e-06, grad_fn=<MseLossBackward0>)\n",
      "968 2.4924715944507625e-06\n",
      "tensor(2.4641e-06, grad_fn=<MseLossBackward0>)\n",
      "969 2.4640708033984993e-06\n",
      "tensor(2.4366e-06, grad_fn=<MseLossBackward0>)\n",
      "970 2.436621571177966e-06\n",
      "tensor(2.4099e-06, grad_fn=<MseLossBackward0>)\n",
      "971 2.409938360870001e-06\n",
      "tensor(2.3827e-06, grad_fn=<MseLossBackward0>)\n",
      "972 2.3827333279768936e-06\n",
      "tensor(2.3568e-06, grad_fn=<MseLossBackward0>)\n",
      "973 2.3568420601804974e-06\n",
      "tensor(2.3298e-06, grad_fn=<MseLossBackward0>)\n",
      "974 2.3298450742004206e-06\n",
      "tensor(2.3034e-06, grad_fn=<MseLossBackward0>)\n",
      "975 2.3033935576677322e-06\n",
      "tensor(2.2782e-06, grad_fn=<MseLossBackward0>)\n",
      "976 2.2781603092880687e-06\n",
      "tensor(2.2526e-06, grad_fn=<MseLossBackward0>)\n",
      "977 2.252578269690275e-06\n",
      "tensor(2.2275e-06, grad_fn=<MseLossBackward0>)\n",
      "978 2.2275087303569308e-06\n",
      "tensor(2.2020e-06, grad_fn=<MseLossBackward0>)\n",
      "979 2.2020324195182184e-06\n",
      "tensor(2.1783e-06, grad_fn=<MseLossBackward0>)\n",
      "980 2.1783221200166736e-06\n",
      "tensor(2.1537e-06, grad_fn=<MseLossBackward0>)\n",
      "981 2.153703007934382e-06\n",
      "tensor(2.1297e-06, grad_fn=<MseLossBackward0>)\n",
      "982 2.129726226485218e-06\n",
      "tensor(2.1059e-06, grad_fn=<MseLossBackward0>)\n",
      "983 2.1058688162156614e-06\n",
      "tensor(2.0828e-06, grad_fn=<MseLossBackward0>)\n",
      "984 2.0828474589507096e-06\n",
      "tensor(2.0593e-06, grad_fn=<MseLossBackward0>)\n",
      "985 2.059268581433571e-06\n",
      "tensor(2.0366e-06, grad_fn=<MseLossBackward0>)\n",
      "986 2.036646719716373e-06\n",
      "tensor(2.0130e-06, grad_fn=<MseLossBackward0>)\n",
      "987 2.0129582480876707e-06\n",
      "tensor(1.9904e-06, grad_fn=<MseLossBackward0>)\n",
      "988 1.9904050532204565e-06\n",
      "tensor(1.9683e-06, grad_fn=<MseLossBackward0>)\n",
      "989 1.9682825040945318e-06\n",
      "tensor(1.9468e-06, grad_fn=<MseLossBackward0>)\n",
      "990 1.946782276718295e-06\n",
      "tensor(1.9246e-06, grad_fn=<MseLossBackward0>)\n",
      "991 1.9245715066062985e-06\n",
      "tensor(1.9034e-06, grad_fn=<MseLossBackward0>)\n",
      "992 1.9034428078157362e-06\n",
      "tensor(1.8818e-06, grad_fn=<MseLossBackward0>)\n",
      "993 1.8818420812749537e-06\n",
      "tensor(1.8610e-06, grad_fn=<MseLossBackward0>)\n",
      "994 1.8610116967465729e-06\n",
      "tensor(1.8406e-06, grad_fn=<MseLossBackward0>)\n",
      "995 1.8406284425509511e-06\n",
      "tensor(1.8195e-06, grad_fn=<MseLossBackward0>)\n",
      "996 1.8195155462308321e-06\n",
      "tensor(1.7990e-06, grad_fn=<MseLossBackward0>)\n",
      "997 1.7990208789342432e-06\n",
      "tensor(1.7795e-06, grad_fn=<MseLossBackward0>)\n",
      "998 1.7795210851545562e-06\n",
      "tensor(1.7595e-06, grad_fn=<MseLossBackward0>)\n",
      "999 1.7595335748410434e-06\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "#     print(epoch, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3376464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99753255]\n",
      " [ 2.997888  ]\n",
      " [ 4.9982433 ]\n",
      " [ 6.9985986 ]\n",
      " [ 8.998955  ]\n",
      " [10.9993105 ]\n",
      " [12.999665  ]\n",
      " [15.000021  ]\n",
      " [17.000376  ]\n",
      " [19.00073   ]\n",
      " [21.001087  ]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a078350",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.],\n",
       "       [ 3.],\n",
       "       [ 5.],\n",
       "       [ 7.],\n",
       "       [ 9.],\n",
       "       [11.],\n",
       "       [13.],\n",
       "       [15.],\n",
       "       [17.],\n",
       "       [19.],\n",
       "       [21.]], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b985cd",
   "metadata": {},
   "source": [
    "### Backward from the scratch\n",
    "- 실제 backward는 Module 단계에서 직접 지정가능(하지만 할필요가 x)\n",
    "- Module에서 backward와 optimizer 오버라이딩\n",
    "- 사용자가 직접 미분 수식을 써야하는 부담감  \n",
    "    $\\rightarrow$ 쓸일은 없으나 순서는 이해할 필요 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099e0039",
   "metadata": {},
   "source": [
    "예시) [로지스틱 회귀 (sigmoid function)](https://colab.research.google.com/drive/1MJF1S8EcRlSjRGHF4XklgMUuMqTQpZHT#scrollTo=pJH8jaQil5gG)\n",
    "\n",
    "$$ h_\\theta(x) = \\frac{1}{1 + e^{z}} $$\n",
    "\n",
    "$$ z = -\\theta^{T} x$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa520d9",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e54f1",
   "metadata": {},
   "source": [
    "- 모델에 데이터를 먹이는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2799df",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_32.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dafcd",
   "metadata": {},
   "source": [
    "1. Data가 있는 폴더가 있다.\n",
    "2. Data set class에서 를 어떻게 시작(init)할건지 길이가 얼만지(len) 어떻게 불러올것인지(getitem > __map-style__ 중요)\n",
    "3. transforms에서 data 변형(tensor로)\n",
    "4. DataLoader : 데이터를 묶어서 model에 feeding해줌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502e635",
   "metadata": {},
   "source": [
    "### Data set 클래스\n",
    "- 데이터 입력 형태를 정의하는 클래스\n",
    "- 데이터를 입력하는 방식의 표준화\n",
    "- Image, Text, Audio등에 따른 입력 정의 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "14d4ee7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    # 초기 데이터 생성 방법 지정\n",
    "    def __init__(self, text, labels):\n",
    "        self.labels = labels\n",
    "        self.data = text\n",
    "    \n",
    "    # 데이터의 전체 길이\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        text = self.data[idx]\n",
    "        sample = {\"Text\" : text, \"Class\" : label}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed6e5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ['Happy', 'Amazing', 'Sad', 'Unhapy', 'Glum']\n",
    "labels = ['Positive', 'Positive', 'Negative', 'Negative', 'Negative']\n",
    "MyDataset = CustomDataset(text, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b10af37f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.CustomDataset"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(MyDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049d743",
   "metadata": {},
   "source": [
    "### Dataset 클래스 생성시 유의점\n",
    "- 데이터 형태에 따라 각 함수를 다르게 정의함\n",
    "- 모든 것을 데이터 생성 시점에 처리할 필요는 없음 : image의 Tensor 변화는 학습에 필요한 시점에 변환\n",
    "- Dataset에 대한 표준화된 처리방법 제공 필요 \n",
    "- 최근엔 HuggingFace등 표준화된 라이브러리 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c48cf",
   "metadata": {},
   "source": [
    "### DataLoader 클래스\n",
    "- Data의 Batch를 생성해주는 클래스\n",
    "- 학습직전(GPU feed전) 데이터의 변환을 책임\n",
    "- Tensor로 변환 + Batch 처리가 메인 업무\n",
    "- 병렬적인 데이터 전처리 코드의 고민 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43105f62",
   "metadata": {},
   "source": [
    "#### 기억해야할 DataLoader의 parameters\n",
    "- sampler, batch_sampler : 데이터를 어떻게 뽑을지 정함\n",
    "- collate_fn : data가 비어잇거나 크기가 다른 Variable을 0으로 채워줌\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e09cb487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Text': ['Amazing', 'Glum'], 'Class': ['Positive', 'Negative']}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MyDataLoader = DataLoader(MyDataset, batch_size=2, shuffle=True)\n",
    "next(iter(MyDataLoader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6225f87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Text': ['Unhapy', 'Sad', 'Amazing'], 'Class': ['Negative', 'Negative', 'Positive']}\n",
      "{'Text': ['Happy', 'Glum'], 'Class': ['Positive', 'Negative']}\n"
     ]
    }
   ],
   "source": [
    "MyDataLoader = DataLoader(MyDataset, batch_size=3, shuffle=True)\n",
    "for dataset in MyDataLoader:\n",
    "    print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7e28",
   "metadata": {},
   "source": [
    "### Casestudy\n",
    "- [실습해보기](https://colab.research.google.com/drive/1Lly2TGJ-L146AAOoQZMTa7S8Waw4ejor#scrollTo=a17b9074)\n",
    "- vision data다운을 받을 수 있음, 직접 생성해보는것 권장\n",
    "- 데이터 다운로드 부터 loader까지 구현해보기\n",
    "- NotMNIST 데이터의 다운로드 자동화 도전|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50556d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
