{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21bd6697",
   "metadata": {},
   "source": [
    "### Key Components of Deep Learning\n",
    "- The **Data** that the model can learn from\n",
    "- The **model** how to transform the data\n",
    "- The **loss** function that quantifies the badness of the model\n",
    "- The **algorithm** to adjust the parameters to minimize the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a956277",
   "metadata": {},
   "source": [
    "### DATA\n",
    "- Data depend on the type of the problem to solve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4354e01a",
   "metadata": {},
   "source": [
    "### Model\n",
    "- Data가 주어졌을 때 바꿔주는 역할\n",
    "- AlxNet, GoogleNet, ResNet, LSTM, GAN등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53abf73a",
   "metadata": {},
   "source": [
    "### Loss\n",
    "- The loss functions is proxy of what we want to achieve\n",
    "- Regression Task (MSE : 제곱 평균 오차)\n",
    "- Classification Task (CE : 교차 엔트로피 )\n",
    "- Probabilistic Task : (MLE : 최대가능도 방법)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b83d400",
   "metadata": {},
   "source": [
    "### Optimization Algorithm 최적화\n",
    "- Data, Model, loss가 주어지면 Net를 어떻게 줄일지\n",
    "- SGD, Momentum, NAG, Adagrad, Adadelta, Rmsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c85ef8b",
   "metadata": {},
   "source": [
    "### Historical Reivew of DL\n",
    "- 2012-AlexNet(cnn) : 224 x 224 image를 분류하는 게 목표. DL이 image를 분류 대회에서 처음 1등을 함. 그 이후로 항상 DL이 1등.\n",
    "- 2013-DQN : 알파고를 만든 알고리즘(Deep mind). Q-learning을 사용함.\n",
    "- 2014-Encoder/Decoder : 다른 언어(한자)의 연속이 주어졌을 때 다른 언어의 연속(영어)으로 추력해주는 것이 목표\n",
    "- 2014-Adam Optimizer : 결과가 잘나온다. GPU가 적은 개인들에게 웬만하면 잘되는 Adam이라는 방법론이 큰 도움이되었다.\n",
    "- 2015-GAN(Generative Adversarial Network) \n",
    "- 2015-Residual Networks : DL이 DL이 가능해지게 해준 논문, 그 전 까지는 DL이 왜 잘 되는지 잘 몰랐다\n",
    "- 2017-Transformer : Attension Is All You Need라는 제목의 논문으로 나옴.\n",
    "- 2018-BERT(fine-tuned NLP models) : NLP는 language model을 학습을 시킨다. 다른 분야의 다양한 단어들로 pre-train 을 하고 하고싶은 분야에 적용\n",
    "- 2019-BIC Language Models : GPT-3 175 billiion parameters가 있다.\n",
    "- 2020 - Self Supervised Learning : cat dog를 분류할 때 cat dog 사진 뿐만 아니라 다른 사진도 활용. train data만 사용하지 않고 classification을 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842da2fd",
   "metadata": {},
   "source": [
    "[레퍼런스](https://dennybritz.com/blog/deep-learning-most-important-ideas/) : 출처블로그"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dcc859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
