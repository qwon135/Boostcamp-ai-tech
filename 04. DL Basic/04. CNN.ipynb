{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dca9407",
   "metadata": {},
   "source": [
    "Convolution\n",
    "- Continuous convolution $\\tau = nT_p$\n",
    "\n",
    "\n",
    "$$ (f \\ast g)(t) = \\int f(\\tau)g(t - \\tau)d\\tau = \\int f(t - \\tau)g(t)d\\tau $$\n",
    "\n",
    "\n",
    "- Discrete convolution\n",
    "\n",
    "\n",
    "$$ (f \\ast g)(t) = \\displaystyle\\sum^\\infty_{i=-\\infty} f(i)g(t - i) = \\displaystyle\\sum^\\infty_{i=-\\infty} f(t - i)g(i) $$\n",
    "\n",
    "\n",
    "- 2D image convolution\n",
    "\n",
    "\n",
    "$$ (I \\ast K)(i,j)=\\displaystyle\\sum_m\\displaystyle\\sum_nI(m,n)K(i-m,j-n) = \\displaystyle\\sum_m\\displaystyle\\sum_nI(i - m,j - n)K(m,n) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f685ff",
   "metadata": {},
   "source": [
    "### Stack of Convolutions\n",
    "\n",
    "\n",
    "32x32x3 이 28x28x4가 되려면 5x5x3 filter가 4개가 필요\n",
    "\n",
    "28x28x4 가 24x24x10이되려면 5x5x4 filter가 10개 필요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c451961",
   "metadata": {},
   "source": [
    "\n",
    "### Convolutional Neural Networks\n",
    "\n",
    "- CNN consists of convolution layer, pooling layer, and fully connected layer.\n",
    "    - Convolution and pooling layer : feature extraction\n",
    "    - Fully connected layer : decision making(e.g., classification)CNN이 발전하는 방향은 같은 모델을 만들고 최대한 deep하게 가져가는 동시에 parameter를 줄이기 위해 여러 tech를 사용.(점점 없어지는 추세)\n",
    "    - 전체 parameter, layer가 몇개인지 감을 가지고 있는게 중요하다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b51f6",
   "metadata": {},
   "source": [
    "### Stride\n",
    "\n",
    "넓게 걷는다는 뜻으로 stride만큼 filter를 찍을지를 의미한다.\n",
    "\n",
    "\n",
    "### Padding\n",
    "\n",
    "가장자리는 찍을수가 없으므로 padding을 덧대어 준다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10bf60",
   "metadata": {},
   "source": [
    "## Mordern CNN - 1x1 convolution의 중요성\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712f6cf9",
   "metadata": {},
   "source": [
    "### ILSVRC\n",
    "\n",
    "이미지넷 이미지셋 훈련대회에 우승 모델들을 알아보자\n",
    "\n",
    "\n",
    "ImageNet Large-Scale Visual Recognition Challenge\n",
    "- Classification/Detection/Localization/Segmentation\n",
    "- 1000 different categories\n",
    "- Over 1 million images\n",
    "- Training set : 계속 늘어난다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561684d",
   "metadata": {},
   "source": [
    "### AlexNet\n",
    "Network가 두개이다.  \n",
    "- 당시 gpu성능이 부족해 2개씩 따로 넣었기때문\n",
    "\n",
    "\n",
    "11x11 filter 사용함\n",
    "- parameters관점에서 11x11 필터는 좋지않다. 볼 수 있는 영역은 커지만 더 많음 parameters가 필요하기 때문이다.\n",
    "\n",
    "#### Key ideas\n",
    "\n",
    "- ReLU를 사용 : Overcome the vanishing gradient problem\n",
    "- 2개의 GPU를 사용\n",
    "- Local response normalization(큰 response는 낮춰줌)\n",
    "- Data augmentation\n",
    "- Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3941ae55",
   "metadata": {},
   "source": [
    "### VGGNet\n",
    "3x3 convolution filters만 사용하였다.\n",
    "\n",
    "**why? 3x3 convolution?**\n",
    "\n",
    "- 3x3 2는 5x5 1개를 사용하는것은 같다. \n",
    "- parameters는 5x5 한개가 훨씬 크다. (5x5x1 > 3x3x2)\n",
    "- 이런 이유로 cnn에선 대부분 7x7을 벗어나지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40e4015",
   "metadata": {},
   "source": [
    "### GoogLeNet\n",
    "\n",
    "\n",
    "중간 중간 1x1 convolution을 사용해 parameters를 줄였다.  \n",
    "NIN구조(Networks in networks)\n",
    "\n",
    "**Inception Block**\n",
    "\n",
    "- 3x3, 5x5이전에 1x1을 넣는다 -> 전체 parameters가 줄어든다.\n",
    "\n",
    "**How?**\n",
    "\n",
    "- Channel 방향으로 dimension을 줄인다.\n",
    "\n",
    "ex)\n",
    "\n",
    "128 $\\rightarrow$ (3x3) $\\rightarrow$ 128 은 3x3x128x128 = 147,456\n",
    "\n",
    "128 $\\rightarrow$ (1x1) $\\rightarrow$ 32 $\\rightarrow$ (3x3) $\\rightarrow$ 128 에 1x1을 하나 끼우면 1x1x128x32 + 3x3x32x128 = 40,960\n",
    "\n",
    "결과는 같지만 1x1 convolution을 사용해 parameters는 거의 1/3으로 줄었다.\n",
    "\n",
    "\n",
    "\n",
    "Quiz)\n",
    "\n",
    "AlexNet 8-layers -> 60m\n",
    "VGGNet 19-layers -> 110m\n",
    "GoogLeNet 22-layers -> 4m\n",
    "3번방법이 가장 prams가적다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c152dd2d",
   "metadata": {},
   "source": [
    "### ResNet\n",
    "\n",
    "Performance가 증가하는 동안 parameter size는 줄어든다.  \n",
    "\n",
    "**Deep neural networks are hard to train**\n",
    "- overfitting is usually caused by an excessive number of parameters.(학습,테스트의 값이 줄어들어도 학습이 잘되는 것은 아니다.)\n",
    "\n",
    "- But, not in this case\n",
    "\n",
    "- Add an identity map(skip connection) : training-test의 차이만 학습하는 것\n",
    "\n",
    "- Simple Shortcut(대부분 사용), Projected Shortcut이 있다.\n",
    "\n",
    "- Bottleneck architecture : inception Blcok이랑 같다. 1x1을앞 뒤에 넣어 input을 줄인다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b77ed7",
   "metadata": {},
   "source": [
    "### Dense Net\n",
    "ResNet을 넣하지 않고 채널이 같으므로 concatenation을한다. 하지만 채널의 크기가 기하급수적으로 커지므로 중간에 1x1Conv 를 넣어 Channel을 줄인다.\n",
    "\n",
    "요약:\n",
    "\n",
    "VGG에서 3x3을 사용하게 되고 GoogLeNet에서 1x1로 크기를 줄이며 RestNet으로 skip-connection을 하며 DenseNet에서 concatenation을 하였다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
