{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d8c4796",
   "metadata": {},
   "source": [
    "### 신경망을 수식으로 분해하기\n",
    "데이터를 선형모델로 해석하는 것이 아닌 비선형모델인 __신경망(neural network)__ 로 배워보자.\n",
    "\n",
    "각 행 벡터 $o_i$ 는 데이터 $x_i$와 가중치 행렬 $W$사이의 행렬곱과 절편 $b$ 벡터의 합으로 표현된다고 가정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d204ff9",
   "metadata": {},
   "source": [
    "$$ o_{np} = x_{nd}W_{dp} + b_{np}$$\n",
    "\n",
    "$d$ 개의 원소를 가진 행벡터 $x$를 $p$개의 원소를 가진 행 행벡터 $o$로 표현하기 위해서  \n",
    "$d$ x $p$의 크기를 가진 가중치 행렬 $W$가 필요하다.\n",
    "\n",
    "이 때 출력차원은 $d$ 에서 $p$로 바뀌게된다.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eda635e",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_1.gif\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b320ee",
   "metadata": {},
   "source": [
    "### 소프트맥스 연산\n",
    "- 소프트맥스(softmax)함수는 __모델의 출력을 확률로 해석__ 할수 있게 변환해주는 연산이다.\n",
    "- 출력 벡터 $o$에 softmax 함수를 합성하면 확률 벡터고 __특정 클래스 k에 속할 확률__ 로 해석 할 수 있다. \n",
    "- __분류 문제__ 를 풀 때 선형모델과 소프트멕스 함수를 결합하여 예측한다.\n",
    "- 하지만 추론을 할 때는 __one-hot 벡터__ 로 최댓값을 가진 주소만 1로 출력하므로 softmax사용 x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a564f998",
   "metadata": {},
   "source": [
    "$$ \\text{softmax}(o) = \\text{softmax}(Wx + b)$$  \n",
    "\n",
    "$$ \\text{softmax}(o) = \\Big(\\frac{\\text{exp}(o_1)}{\\sum_{k=1}^{p} \\text{exp}(o_k)} ,..., \\frac{\\text{exp}(o_p)}{\\sum_{k=1}^{p} \\text{exp}(o_k)}\\Big) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "530efc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "denumerator 각 출력함수에 지수함수 적용: \n",
      "  [[3.67879441e-01 1.00000000e+00 1.35335283e-01]\n",
      " [1.35335283e-01 3.67879441e-01 1.00000000e+00]\n",
      " [2.06115362e-09 4.53999298e-05 1.00000000e+00]]\n",
      "\n",
      "numerator   : \n",
      "  [[1.50321472]\n",
      " [1.50321472]\n",
      " [1.0000454 ]]\n",
      "\n",
      "softmax : \n",
      "  [[2.44728471e-01 6.65240956e-01 9.00305732e-02]\n",
      " [9.00305732e-02 2.44728471e-01 6.65240956e-01]\n",
      " [2.06106005e-09 4.53978686e-05 9.99954600e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(vec):\n",
    "    # 너무 큰 vector가 들어오면 overflow가 발생하므로 max값을 벡터에 빼준다.\n",
    "    denumerator = np.exp(vec - np.max(vec, axis = 1, keepdims = True))\n",
    "    print(\"denumerator 각 출력함수에 지수함수 적용: \\n \", denumerator)\n",
    "    numerator = np.sum(denumerator, axis = 1, keepdims = True)\n",
    "    print()\n",
    "    print(\"numerator   : \\n \", numerator)\n",
    "    print()\n",
    "    val = denumerator/numerator\n",
    "    return val\n",
    "vec = np.array([[1,2,0], [-1, 0, 1],[-10, 0, 10]])\n",
    "\n",
    "print(\"softmax : \\n \",softmax(vec))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a7ce77",
   "metadata": {},
   "source": [
    "신경망은 __선형모델과 활성함수(activation function)을 합성한 함수__ 이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47bfda",
   "metadata": {},
   "source": [
    "$$ z = Wx + b$$\n",
    "\n",
    "### $$H = (\\sigma(z_1),...,\\sigma(z_n)), \\sigma(z) = \\sigma(Wx + b)$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e95c849",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_2.gif\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41825da",
   "metadata": {},
   "source": [
    "### 활성함수\n",
    "- 활성함수(acitvation function)은 ℝ 위에 정의된 __비선형(nolinear) 함수__ 로서 딥러닝에서 중요한개념\n",
    "- __활성함수를 쓰지 않으면 딥러닝과 선형모형은 차이가 없다.__\n",
    "- Sigmoid, tanh함수는 전통적으로 많이 쓰이던 활성함수 이나 __딥러닝에선 ReLU가 함수 많이 쓰임__\n",
    "- 활성함수는 각 벡터들에 개별적으로 적용된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a146fda",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_3.gif\" width = 800>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b62c15",
   "metadata": {},
   "source": [
    "신경망은 __선형모델과 활성함수(activation function)을 합성한 함수__ 이다.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328c10b",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_4.gif\" width = 300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296a5e0",
   "metadata": {},
   "source": [
    "$$O = Z^{(L)}$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$H^{(2)} = \\sigma(Z^{(2)})$$\n",
    "$$Z^{(2)} = H^{(1)}W^{(2)} + b{(2)}$$\n",
    "$$.$$\n",
    "$$.$$\n",
    "$$H^{(1)} = \\sigma(Z^{(1)})$$\n",
    "$$Z^{(1)} = XW^{(1)} + b{(1)}$$\n",
    "\n",
    "- H의 성분들은 Z와 모양은 같고 활성함수 적용 여부의 차이가 있다.\n",
    "- MLP의 패러미터는 $L$개의 가중치 행렬 $W^{(L)}...W^{(1)}$과 $b^{(L)}...b^{(1)}$로 이루어져 있다.\n",
    "- 이 순차적인 신경망 계산을 순전파(forward propagation)이라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85094b22",
   "metadata": {},
   "source": [
    "__층을 여러개 쌓는 이유__\n",
    "- 이론적으로는 2층 신경망도 임이의 연속함수 근사가 가능\n",
    "- 그러나 층이 깊을 수록 목적함수를 근사하는데 필요한 노드가 빨리줄어 효율적인 학습이 가능\n",
    "- 하지만 층이 깊다고 최적화가 쉬워지는 것은 아니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db0824b",
   "metadata": {},
   "source": [
    "### 딥러닝 학습 원리 : 역전파 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df4b763",
   "metadata": {},
   "source": [
    "- 딥러닝은  __역전파(backpropagation) 알고리즘__ 을 이용하여 각층에 사용된 패러미터 $\\{W^{(l)}, b^{(l)}\\}_{l=1}^{L}$ 를 학습한다.\n",
    "- 각각의 가중치를 학습시킬 때 가중치에 대한 Gradient Vector를 계산해야 경사하강법 적용이 가능하다.\n",
    "- 손실함수를 $L$ 이라고 했을 때 역전파는 $\\frac{\\partial L}{\\partial W^{(l)}}$ 정보를 계산할 때 사용한다.\n",
    "- 선형모델은 한 층 에서만 계산하므로 동시 적용이 가능하지만 딥러닝은 동시적용이 불가능하다.\n",
    "- 각 층 패러미터의 Gradient Vector는 윗층부터 역순으로 계산하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23232180",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_5.gif\" width = 700>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de01180",
   "metadata": {},
   "source": [
    "### 역전파 알고리즘의 원리 이해\n",
    "- 역전파 알고리즘은 합성함수 미분법인 __연쇄법칙(chain-rule)기반 자동 미분(auto-defferentiation)__ 을 사용 한다.\n",
    "- 연쇄법칙을 통해 합성함수의 미분 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea2e3ba",
   "metadata": {},
   "source": [
    "$$ z = (x + y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5614a613",
   "metadata": {},
   "source": [
    "$$z = w^2  $$\n",
    "$$w = x + y$$\n",
    "$$ $$\n",
    "$$\\frac{\\partial z}{\\partial w} = 2w$$\n",
    "$$ $$\n",
    "$$\\frac{\\partial w}{\\partial x} = 1, \\frac{\\partial w}{\\partial y} = 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8347d3",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial w}\\frac{\\partial w}{\\partial x}= 2w * 1 = 2(x + y)$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecbb93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
