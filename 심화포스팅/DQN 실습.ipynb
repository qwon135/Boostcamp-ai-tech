{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b037fa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "env = gym.make('CartPole-v0').unwrapped\n",
    "\n",
    "# matplotlib 설정\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "# GPU를 사용할 경우\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2bce90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([],maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"transition 저장\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa36ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, h, w, outputs):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Linear 입력의 연결 숫자는 conv2d 계층의 출력과 입력 이미지의 크기에\n",
    "        # 따라 결정되기 때문에 따로 계산을 해야합니다.\n",
    "        def conv2d_size_out(size, kernel_size = 5, stride = 2):\n",
    "            return (size - (kernel_size - 1) - 1) // stride  + 1\n",
    "        convw = conv2d_size_out(conv2d_size_out(conv2d_size_out(w)))\n",
    "        convh = conv2d_size_out(conv2d_size_out(conv2d_size_out(h)))\n",
    "        linear_input_size = convw * convh * 32\n",
    "        self.head = nn.Linear(linear_input_size, outputs)\n",
    "\n",
    "    # 최적화 중에 다음 행동을 결정하기 위해서 하나의 요소 또는 배치를 이용해 호촐됩니다.\n",
    "    # ([[left0exp,right0exp]...]) 를 반환합니다.\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356497e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "resize = T.Compose([T.ToPILImage(),\n",
    "                    T.Resize(40, interpolation=torchvision.transforms.InterpolationMode.BICUBIC),\n",
    "                    T.ToTensor()])\n",
    "\n",
    "\n",
    "def get_cart_location(screen_width):\n",
    "    world_width = env.x_threshold * 2\n",
    "    scale = screen_width / world_width\n",
    "    return int(env.state[0] * scale + screen_width / 2.0)  # MIDDLE OF CART\n",
    "\n",
    "def get_screen():\n",
    "    # gym이 요청한 화면은 400x600x3 이지만, 가끔 800x1200x3 처럼 큰 경우가 있습니다.\n",
    "    # 이것을 Torch order (CHW)로 변환한다.\n",
    "    screen = env.render(mode='rgb_array').transpose((2, 0, 1))\n",
    "    # 카트는 아래쪽에 있으므로 화면의 상단과 하단을 제거하십시오.\n",
    "    _, screen_height, screen_width = screen.shape\n",
    "    screen = screen[:, int(screen_height*0.4):int(screen_height * 0.8)]\n",
    "    view_width = int(screen_width * 0.6)\n",
    "    cart_location = get_cart_location(screen_width)\n",
    "    if cart_location < view_width // 2:\n",
    "        slice_range = slice(view_width)\n",
    "    elif cart_location > (screen_width - view_width // 2):\n",
    "        slice_range = slice(-view_width, None)\n",
    "    else:\n",
    "        slice_range = slice(cart_location - view_width // 2,\n",
    "                            cart_location + view_width // 2)\n",
    "    # 카트를 중심으로 정사각형 이미지가 되도록 가장자리를 제거하십시오.\n",
    "    screen = screen[:, :, slice_range]\n",
    "    # float 으로 변환하고,  rescale 하고, torch tensor 로 변환하십시오.\n",
    "    # (이것은 복사를 필요로하지 않습니다)\n",
    "    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
    "    screen = torch.from_numpy(screen)\n",
    "    # 크기를 수정하고 배치 차원(BCHW)을 추가하십시오.\n",
    "    return resize(screen).unsqueeze(0)\n",
    "\n",
    "\n",
    "env.reset()\n",
    "plt.figure()\n",
    "plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),\n",
    "           interpolation='none')\n",
    "plt.title('Example extracted screen')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89b9d689",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.999\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 200\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "# AI gym에서 반환된 형태를 기반으로 계층을 초기화 하도록 화면의 크기를\n",
    "# 가져옵니다. 이 시점에 일반적으로 3x40x90 에 가깝습니다.\n",
    "# 이 크기는 get_screen()에서 고정, 축소된 렌더 버퍼의 결과입니다.\n",
    "init_screen = get_screen()\n",
    "_, _, screen_height, screen_width = init_screen.shape\n",
    "\n",
    "# gym 행동 공간에서 행동의 숫자를 얻습니다.\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "policy_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net = DQN(screen_height, screen_width, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.RMSprop(policy_net.parameters())\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "\n",
    "steps_done = 0\n",
    "\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "        math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max (1)은 각 행의 가장 큰 열 값을 반환합니다.\n",
    "            # 최대 결과의 두번째 열은 최대 요소의 주소값이므로,\n",
    "            # 기대 보상이 더 큰 행동을 선택할 수 있습니다.\n",
    "            return policy_net(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "episode_durations = []\n",
    "\n",
    "\n",
    "def plot_durations():\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # 100개의 에피소드 평균을 가져 와서 도표 그리기\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # 도표가 업데이트되도록 잠시 멈춤\n",
    "    if is_ipython:\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f20130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). 이것은 batch-array의 Transitions을 Transition의 batch-arrays로\n",
    "    # 전환합니다.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # 최종이 아닌 상태의 마스크를 계산하고 배치 요소를 연결합니다\n",
    "    # (최종 상태는 시뮬레이션이 종료 된 이후의 상태)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # Q(s_t, a) 계산 - 모델이 Q(s_t)를 계산하고, 취한 행동의 열을 선택합니다.\n",
    "    # 이들은 policy_net에 따라 각 배치 상태에 대해 선택된 행동입니다.\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "    # 모든 다음 상태를 위한 V(s_{t+1}) 계산\n",
    "    # non_final_next_states의 행동들에 대한 기대값은 \"이전\" target_net을 기반으로 계산됩니다.\n",
    "    # max(1)[0]으로 최고의 보상을 선택하십시오.\n",
    "    # 이것은 마스크를 기반으로 병합되어 기대 상태 값을 갖거나 상태가 최종인 경우 0을 갖습니다.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()\n",
    "    # 기대 Q 값 계산\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Huber 손실 계산\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # 모델 최적화\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d6e4dc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes = 50\n",
    "for i_episode in range(num_episodes):\n",
    "    # 환경과 상태 초기화\n",
    "    env.reset()\n",
    "    last_screen = get_screen()\n",
    "    current_screen = get_screen()\n",
    "    state = current_screen - last_screen\n",
    "    for t in count():\n",
    "        # 행동 선택과 수행\n",
    "        action = select_action(state)\n",
    "        _, reward, done, _ = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=device)\n",
    "\n",
    "        # 새로운 상태 관찰\n",
    "        last_screen = current_screen\n",
    "        current_screen = get_screen()\n",
    "        if not done:\n",
    "            next_state = current_screen - last_screen\n",
    "        else:\n",
    "            next_state = None\n",
    "\n",
    "        # 메모리에 변이 저장\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # 다음 상태로 이동\n",
    "        state = next_state\n",
    "\n",
    "        # (정책 네트워크에서) 최적화 한단계 수행\n",
    "        optimize_model()\n",
    "        if done:\n",
    "            episode_durations.append(t + 1)\n",
    "            plot_durations()\n",
    "            break\n",
    "    # 목표 네트워크 업데이트, 모든 웨이트와 바이어스 복사\n",
    "    if i_episode % TARGET_UPDATE == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "print('Complete')\n",
    "env.render()\n",
    "env.close()\n",
    "plt.ioff()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afb4fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
