{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b43d3c2a",
   "metadata": {},
   "source": [
    "#  AutoGrad & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95044ed7",
   "metadata": {},
   "source": [
    "논문 구현 : 수 많은 반복의 연속  \n",
    "Layer = Block \n",
    "Layer를 쌓는것(블록의 연속)을 다시 Layer에 넣기도함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb757909",
   "metadata": {},
   "source": [
    "### torch.nn.Module\n",
    "- 딥러닝을 구성하는 Layer의 base class(Auto Grad)\n",
    "- 4가지를 정함 : Input, Output, Forward, Backward(Weights)\n",
    "- 학습의 대상이 되는 parameter(tensor) 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b272df",
   "metadata": {},
   "source": [
    "### nn.Parameter\n",
    "- Tensor 객체의 상속 객체\n",
    "- nn.Module 내에서 __attribute__ 가 될 때는 required_grad=True(Auto Grad)로 지정되어 학습 대상이 되는 Tensor\n",
    "- 직접 지정할 일은 잘 없다 : 대부분의 layer에는 weights 값들이 지정되어 잇다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86e6e05",
   "metadata": {},
   "source": [
    "### Backward\n",
    "- Layer에 있는 Parameter 들의 미분을 수행\n",
    "- Forward의 결과값(model의 output=예측치)과 실제값간의 loss에 대해 미분\n",
    "- 해당 값으로 Parameter 업데이트\n",
    "- 총 4단계\n",
    " 1. optimizer.zero_grad() \n",
    " 2. loss = criterion(outputs, labels)\n",
    " 3. loss.backward()\n",
    " 4. optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b985cd",
   "metadata": {},
   "source": [
    "### Backward from the scratch\n",
    "- 실제 backward는 Module 단계에서 직접 지정가능(하지만 할필요가 x)\n",
    "- Module에서 backward와 optimizer 오버라이딩\n",
    "- 사용자가 직접 미분 수식을 써야하는 부담감  \n",
    "    $\\rightarrow$ 쓸일은 없으나 순서는 이해할 필요 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa520d9",
   "metadata": {},
   "source": [
    "# PyTorch Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90e54f1",
   "metadata": {},
   "source": [
    "- 모델에 데이터를 먹이는 법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2799df",
   "metadata": {},
   "source": [
    "<img src = \"../images/ai_32.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577dafcd",
   "metadata": {},
   "source": [
    "1. Data가 있는 폴더가 있다.\n",
    "2. Data set class에서 를 어떻게 시작(init)할건지 길이가 얼만지(len) 어떻게 불러올것인지(getitem > __map-style__ 중요)\n",
    "3. transforms에서 data 변형(tensor로)\n",
    "4. DataLoader : 데이터를 묶어서 model에 feeding해줌\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f502e635",
   "metadata": {},
   "source": [
    "### Data set 클래스\n",
    "- 데이터 입력 형태를 정의하는 클래스\n",
    "- 데이터를 입력하는 방식의 표준화\n",
    "- Image, Text, Audio등에 따른 입력 정의 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e049d743",
   "metadata": {},
   "source": [
    "### Dataset 클래스 생성시 유의점\n",
    "- 데이터 형태에 따라 각 함수를 다르게 정의함\n",
    "- 모든 것을 데이터 생성 시점에 처리할 필요는 없음 : image의 Tensor 변화는 학습에 필요한 시점에 변환\n",
    "- Dataset에 대한 표준화된 처리방법 제공 필요 \n",
    "- 최근엔 HuggingFace등 표준화된 라이브러리 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24c48cf",
   "metadata": {},
   "source": [
    "### DataLoader 클래스\n",
    "- Data의 Batch를 생성해주는 클래스\n",
    "- 학습직전(GPU feed전) 데이터의 변환을 책임\n",
    "- Tensor로 변환 + Batch 처리가 메인 업무\n",
    "- 병렬적인 데이터 전처리 코드의 고민 필요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71d7e28",
   "metadata": {},
   "source": [
    "### Casestudy\n",
    "- 데이터 다운로드 부터 loader까지 구현해보기\n",
    "- NotMNIST 데이터의 다운로드 자동화 도전|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e50556d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
